import os


class ModelConfig:
    """
    –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏ –∏ –ø—É—Ç–µ–π –∫ —Ñ–∞–π–ª–∞–º
    """
    
    # ==================== –ü–ê–†–ê–ú–ï–¢–†–´ –ú–û–î–ï–õ–ò ====================
    
    # –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤)
    VOCAB_SIZE = 5000
    
    # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–≤–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å–ª–æ–≤)
    EMBEDDING_DIM = 256
    
    # –†–∞–∑–º–µ—Ä —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è LSTM
    HIDDEN_SIZE = 512
    
    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ—ë–≤ LSTM
    NUM_LAYERS = 2
    
    # Dropout –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ (–ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è)
    DROPOUT = 0.3
    
    # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (–≤ —Ç–æ–∫–µ–Ω–∞—Ö)
    MAX_SEQ_LENGTH = 100
    
    # ==================== –ü–ê–†–ê–ú–ï–¢–†–´ –û–ë–£–ß–ï–ù–ò–Ø ====================
    
    # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –∑–∞ –æ–¥–Ω—É –∏—Ç–µ—Ä–∞—Ü–∏—é)
    BATCH_SIZE = 32
    
    # Learning rate (—Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è)
    LEARNING_RATE = 0.001
    
    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è
    NUM_EPOCHS = 10
    
    # Gradient clipping (–æ–±—Ä–µ–∑–∫–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏)
    GRAD_CLIP = 5.0
    
    # Teacher forcing ratio (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏)
    TEACHER_FORCING_RATIO = 0.5
    
    # ==================== –ü–£–¢–ò –ö –§–ê–ô–õ–ê–ú ====================
    
    # –ë–∞–∑–æ–≤—ã–π –ø—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É
    BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
    
    # –ü—É—Ç—å –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É FAQ –¥–∞—Ç–∞—Å–µ—Ç—É
    FAQ_SOURCE_PATH = os.path.join(BASE_DIR, "data", "faq_70_questions.json")
    
    # –ü—É—Ç—å –∫ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    DATA_PATH = os.path.join(BASE_DIR, "data", "training", "faq_dataset.json")
    
    # –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
    MODEL_SAVE_PATH = os.path.join(BASE_DIR, "ml", "models", "chatbot_model.pt")
    
    # –ü—É—Ç—å –∫ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—É
    TOKENIZER_PATH = os.path.join(BASE_DIR, "ml", "models", "tokenizer.pkl")
    
    # –ü—É—Ç—å –∫ —á–µ–∫–ø–æ–∏–Ω—Ç–∞–º (–ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏)
    CHECKPOINT_DIR = os.path.join(BASE_DIR, "ml", "models", "checkpoints")
    
    # ==================== –ü–ê–†–ê–ú–ï–¢–†–´ –î–õ–Ø ruGPT-3 ====================
    
    # –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è fine-tuning
    RUGPT_MODEL_NAME = "sberbank-ai/rugpt3small_based_on_gpt2"
    
    # –ü—É—Ç—å –∫ fine-tuned –º–æ–¥–µ–ª–∏
    RUGPT_SAVE_PATH = os.path.join(BASE_DIR, "ml", "models", "rugpt3_finetuned")
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è ruGPT
    RUGPT_BATCH_SIZE = 8
    RUGPT_LEARNING_RATE = 5e-5
    RUGPT_NUM_EPOCHS = 3
    RUGPT_MAX_LENGTH = 256
    
    # ==================== –£–°–¢–†–û–ô–°–¢–í–û ====================
    
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ (CUDA –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞, –∏–Ω–∞—á–µ CPU)
    DEVICE = "cuda"  # –ë—É–¥–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
    
    # ==================== –ú–ï–¢–†–ò–ö–ò –ö–ê–ß–ï–°–¢–í–ê ====================
    
    # –¶–µ–ª–µ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ accuracy (—Ç–æ—á–Ω–æ—Å—Ç—å) >= 70%
    TARGET_ACCURACY = 0.70
    
    # –¶–µ–ª–µ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ BLEU score >= 0.5
    TARGET_BLEU = 0.50
    
    # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ perplexity (—á–µ–º –Ω–∏–∂–µ, —Ç–µ–º –ª—É—á—à–µ)
    MAX_PERPLEXITY = 50.0
    
    @classmethod
    def print_config(cls):
        """–í—ã–≤–æ–¥ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –≤ –∫–æ–Ω—Å–æ–ª—å"""
        print("=" * 60)
        print("–ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –ú–û–î–ï–õ–ò –ß–ê–¢-–ë–û–¢–ê")
        print("=" * 60)
        print(f"–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {cls.VOCAB_SIZE}")
        print(f"–≠–º–±–µ–¥–¥–∏–Ω–≥: {cls.EMBEDDING_DIM}D")
        print(f"LSTM hidden: {cls.HIDDEN_SIZE}")
        print(f"–°–ª–æ–∏: {cls.NUM_LAYERS}")
        print(f"Dropout: {cls.DROPOUT}")
        print(f"Max length: {cls.MAX_SEQ_LENGTH}")
        print(f"\n–ë–∞—Ç—á: {cls.BATCH_SIZE}")
        print(f"Learning rate: {cls.LEARNING_RATE}")
        print(f"–≠–ø–æ—Ö–∏: {cls.NUM_EPOCHS}")
        print(f"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {cls.DEVICE}")
        print("=" * 60)


class TrainingConfig:
    """
    –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è
    """
    
    # –°–æ—Ö—Ä–∞–Ω—è—Ç—å –º–æ–¥–µ–ª—å –∫–∞–∂–¥—ã–µ N —ç–ø–æ—Ö
    SAVE_EVERY = 2
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞–∂–¥—ã–µ N –±–∞—Ç—á–µ–π
    VALIDATE_EVERY = 100
    
    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥—ã–µ N –±–∞—Ç—á–µ–π
    LOG_EVERY = 50
    
    # Early stopping (–æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –µ—Å–ª–∏ –Ω–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è)
    EARLY_STOPPING_PATIENCE = 3
    
    # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è
    MIN_DELTA = 0.001


if __name__ == "__main__":
    # –¢–µ—Å—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    ModelConfig.print_config()
    print(f"\nüìÅ –ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º: {ModelConfig.DATA_PATH}")
    print(f"üíæ –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏: {ModelConfig.MODEL_SAVE_PATH}")
