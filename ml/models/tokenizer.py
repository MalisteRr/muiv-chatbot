"""
–¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤—ã–ø–æ–ª–Ω—è–µ—Ç:
1. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è –∏–∑ –æ–±—É—á–∞—é—â–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤
2. –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–Ω–¥–µ–∫—Å–æ–≤
3. –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –æ–±—Ä–∞—Ç–Ω–æ –≤ —Ç–µ–∫—Å—Ç
4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ/–∑–∞–≥—Ä—É–∑–∫–∞ –∏–∑ —Ñ–∞–π–ª–∞
"""

import pickle
import re
from collections import Counter
from typing import List, Dict, Tuple


class SimpleTokenizer:
    """
    –ü—Ä–æ—Å—Ç–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    """
    
    # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
    PAD_TOKEN = '<PAD>'    # –ü–∞–¥–¥–∏–Ω–≥ (–∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–æ –Ω—É–∂–Ω–æ–π –¥–ª–∏–Ω—ã)
    UNK_TOKEN = '<UNK>'    # –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ —Å–ª–æ–≤–æ
    SOS_TOKEN = '<SOS>'    # Start of sequence (–Ω–∞—á–∞–ª–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏)
    EOS_TOKEN = '<EOS>'    # End of sequence (–∫–æ–Ω–µ—Ü –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏)
    
    def __init__(self, vocab_size: int = 5000):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        
        Args:
            vocab_size: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è
        """
        self.vocab_size = vocab_size
        
        # –°–ª–æ–≤–∞—Ä–∏ –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Å–ª–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å—ã –∏ –æ–±—Ä–∞—Ç–Ω–æ
        self.word2idx = {
            self.PAD_TOKEN: 0,
            self.UNK_TOKEN: 1,
            self.SOS_TOKEN: 2,
            self.EOS_TOKEN: 3
        }
        self.idx2word = {
            0: self.PAD_TOKEN,
            1: self.UNK_TOKEN,
            2: self.SOS_TOKEN,
            3: self.EOS_TOKEN
        }
        
        # –°—á—ë—Ç—á–∏–∫ —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤
        self.word_count = Counter()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.num_words = 4  # –ù–∞—á–∏–Ω–∞–µ–º —Å 4 —Å–ø–µ—Ü —Ç–æ–∫–µ–Ω–æ–≤
    
    def preprocess(self, text: str) -> List[str]:
        """
        –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
        
        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤ (—Å–ª–æ–≤)
        """
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        text = text.lower()
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        text = ' '.join(text.split())
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        text = re.sub(r'([.,!?;:])', r' \1 ', text)
        
        # –£–±–∏—Ä–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –∫—Ä–æ–º–µ –±—É–∫–≤, —Ü–∏—Ñ—Ä –∏ –æ—Å–Ω–æ–≤–Ω–æ–π –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
        text = re.sub(r'[^–∞-—è—ëa-z0-9\s.,!?;:\-]', '', text)
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Ç–æ–∫–µ–Ω—ã
        tokens = text.split()
        
        return tokens
    
    def build_vocab(self, texts: List[str]):
        """
        –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ—Ä–ø—É—Å–∞ —Ç–µ–∫—Å—Ç–æ–≤
        
        Args:
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        """
        print(f"\nüî® –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è –∏–∑ {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤...")
        
        # –ü–æ–¥—Å—á—ë—Ç —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤
        for idx, text in enumerate(texts, 1):
            tokens = self.preprocess(text)
            self.word_count.update(tokens)
            
            if idx % 100 == 0:
                print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {idx}/{len(texts)} —Ç–µ–∫—Å—Ç–æ–≤...")
        
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤ –Ω–∞–π–¥–µ–Ω–æ: {len(self.word_count)}")
        
        # –ë–µ—Ä—ë–º —Ç–æ–ø N —Å–∞–º—ã—Ö —á–∞—Å—Ç–æ—Ç–Ω—ã—Ö —Å–ª–æ–≤
        most_common = self.word_count.most_common(self.vocab_size - 4)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ª–æ–≤–∞—Ä—å
        for word, freq in most_common:
            if word not in self.word2idx:
                idx = self.num_words
                self.word2idx[word] = idx
                self.idx2word[idx] = word
                self.num_words += 1
        
        print(f"‚úÖ –°–ª–æ–≤–∞—Ä—å –ø–æ—Å—Ç—Ä–æ–µ–Ω: {self.num_words} —Å–ª–æ–≤")
        print(f"   –ü–æ–∫—Ä—ã—Ç–∏–µ: {len(most_common)}/{len(self.word_count)} "
              f"({len(most_common)/len(self.word_count)*100:.1f}%)")
    
    def encode(self, text: str, max_length: int = 100, 
               add_sos: bool = False, add_eos: bool = False) -> List[int]:
        """
        –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–Ω–¥–µ–∫—Å–æ–≤
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
            max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            add_sos: –î–æ–±–∞–≤–∏—Ç—å —Ç–æ–∫–µ–Ω –Ω–∞—á–∞–ª–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            add_eos: –î–æ–±–∞–≤–∏—Ç—å —Ç–æ–∫–µ–Ω –∫–æ–Ω—Ü–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            
        Returns:
            –°–ø–∏—Å–æ–∫ –∏–Ω–¥–µ–∫—Å–æ–≤
        """
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        tokens = self.preprocess(text)
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –∏–Ω–¥–µ–∫—Å—ã (–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Å–ª–æ–≤–∞ -> UNK)
        indices = [self.word2idx.get(token, 1) for token in tokens]
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
        if add_sos:
            indices = [2] + indices  # 2 = <SOS>
        if add_eos:
            indices = indices + [3]  # 3 = <EOS>
        
        # –û–±—Ä–µ–∑–∫–∞ –∏–ª–∏ –ø–∞–¥–¥–∏–Ω–≥ –¥–æ max_length
        if len(indices) < max_length:
            # –î–æ–ø–æ–ª–Ω—è–µ–º –ø–∞–¥–¥–∏–Ω–≥–æ–º
            indices += [0] * (max_length - len(indices))
        else:
            # –û–±—Ä–µ–∑–∞–µ–º (–æ—Å—Ç–∞–≤–ª—è–µ–º –º–µ—Å—Ç–æ –¥–ª—è EOS –µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
            if add_eos and len(indices) > max_length:
                indices = indices[:max_length-1] + [3]
            else:
                indices = indices[:max_length]
        
        return indices
    
    def encode_batch(self, texts: List[str], max_length: int = 100,
                     add_sos: bool = False, add_eos: bool = False) -> List[List[int]]:
        """
        –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –±–∞—Ç—á–∞ —Ç–µ–∫—Å—Ç–æ–≤
        
        Args:
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤
            max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞
            add_sos: –î–æ–±–∞–≤–∏—Ç—å SOS —Ç–æ–∫–µ–Ω
            add_eos: –î–æ–±–∞–≤–∏—Ç—å EOS —Ç–æ–∫–µ–Ω
            
        Returns:
            –°–ø–∏—Å–æ–∫ –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
        """
        return [self.encode(text, max_length, add_sos, add_eos) for text in texts]
    
    def decode(self, indices: List[int], skip_special: bool = True) -> str:
        """
        –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–Ω–¥–µ–∫—Å–æ–≤ –≤ —Ç–µ–∫—Å—Ç
        
        Args:
            indices: –°–ø–∏—Å–æ–∫ –∏–Ω–¥–µ–∫—Å–æ–≤
            skip_special: –ü—Ä–æ–ø—É—Å–∫–∞—Ç—å –ª–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
            
        Returns:
            –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å
        special_tokens = {self.PAD_TOKEN, self.SOS_TOKEN, self.EOS_TOKEN}
        
        words = []
        for idx in indices:
            # –ü–æ–ª—É—á–∞–µ–º —Å–ª–æ–≤–æ –ø–æ –∏–Ω–¥–µ–∫—Å—É
            word = self.idx2word.get(idx, self.UNK_TOKEN)
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if skip_special and word in special_tokens:
                continue
            
            # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è –Ω–∞ EOS –µ—Å–ª–∏ –≤—Å—Ç—Ä–µ—Ç–∏–ª–∏
            if word == self.EOS_TOKEN:
                break
            
            words.append(word)
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç
        text = ' '.join(words)
        
        # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã –ø–µ—Ä–µ–¥ –∑–Ω–∞–∫–∞–º–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        text = re.sub(r'\s+([.,!?;:])', r'\1', text)
        
        return text
    
    def decode_batch(self, indices_batch: List[List[int]], 
                     skip_special: bool = True) -> List[str]:
        """
        –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –±–∞—Ç—á–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
        
        Args:
            indices_batch: –°–ø–∏—Å–æ–∫ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –∏–Ω–¥–µ–∫—Å–æ–≤
            skip_special: –ü—Ä–æ–ø—É—Å–∫–∞—Ç—å —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
            
        Returns:
            –°–ø–∏—Å–æ–∫ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
        """
        return [self.decode(indices, skip_special) for indices in indices_batch]
    
    def get_vocab_size(self) -> int:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è"""
        return self.num_words
    
    def save(self, filepath: str):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –≤ —Ñ–∞–π–ª
        
        Args:
            filepath: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
        """
        with open(filepath, 'wb') as f:
            pickle.dump(self, f)
        print(f"üíæ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {filepath}")
        print(f"   –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {self.num_words} —Å–ª–æ–≤")
    
    @staticmethod
    def load(filepath: str) -> 'SimpleTokenizer':
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏–∑ —Ñ–∞–π–ª–∞
        
        Args:
            filepath: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
            
        Returns:
            –ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        """
        with open(filepath, 'rb') as f:
            tokenizer = pickle.load(f)
        print(f"‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω: {filepath}")
        print(f"   –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {tokenizer.num_words} —Å–ª–æ–≤")
        return tokenizer
    
    def __len__(self) -> int:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è"""
        return self.num_words
    
    def __repr__(self) -> str:
        """–°—Ç—Ä–æ–∫–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞"""
        return f"SimpleTokenizer(vocab_size={self.num_words})"


if __name__ == "__main__":
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
    print("\n" + "=" * 60)
    print("–¢–ï–°–¢ –¢–û–ö–ï–ù–ò–ó–ê–¢–û–†–ê")
    print("=" * 60)
    
    # –°–æ–∑–¥–∞—ë–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    tokenizer = SimpleTokenizer(vocab_size=100)
    
    # –ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–æ–≤
    texts = [
        "–°–∫–æ–ª—å–∫–æ —Å—Ç–æ–∏—Ç –æ–±—É—á–µ–Ω–∏–µ –≤ –ú–£–ò–í?",
        "–ö–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω—É–∂–Ω—ã –¥–ª—è –ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏—è?",
        "–ï—Å—Ç—å –ª–∏ –±—é–¥–∂–µ—Ç–Ω—ã–µ –º–µ—Å—Ç–∞ –Ω–∞ IT –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è—Ö?",
        "–ö–∞–∫ –ø–æ–¥–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã –æ–Ω–ª–∞–π–Ω?"
    ]
    
    # –°—Ç—Ä–æ–∏–º —Å–ª–æ–≤–∞—Ä—å
    tokenizer.build_vocab(texts * 10)  # –ü–æ–≤—Ç–æ—Ä—è–µ–º –¥–ª—è —á–∞—Å—Ç–æ—Ç—ã
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
    test_text = "–°–∫–æ–ª—å–∫–æ —Å—Ç–æ–∏—Ç –æ–±—É—á–µ–Ω–∏–µ?"
    print(f"\nüìù –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: {test_text}")
    
    encoded = tokenizer.encode(test_text, max_length=20, add_sos=True, add_eos=True)
    print(f"üî¢ –ó–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–æ: {encoded}")
    
    decoded = tokenizer.decode(encoded)
    print(f"üìù –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–æ: {decoded}")
    
    print("\n" + "=" * 60)
