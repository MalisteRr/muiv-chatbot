"""
Trainer –¥–ª—è –æ–±—É—á–µ–Ω–∏—è Seq2Seq –º–æ–¥–µ–ª–∏

–ö–ª–∞—Å—Å Trainer —É–ø—Ä–∞–≤–ª—è–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å–æ–º –æ–±—É—á–µ–Ω–∏—è:
1. –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ (forward pass)
2. –í—ã—á–∏—Å–ª–µ–Ω–∏–µ loss
3. –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ (backpropagation)
4. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤
5. –í–∞–ª–∏–¥–∞—Ü–∏—è
6. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import time
import os
from typing import Optional, Dict

from .seq2seq import Seq2Seq
from .config import ModelConfig, TrainingConfig


class Trainer:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è Seq2Seq –º–æ–¥–µ–ª–∏"""
    
    def __init__(
        self,
        model: Seq2Seq,
        optimizer: optim.Optimizer,
        criterion: nn.Module,
        device: str = 'cpu',
        grad_clip: float = 5.0
    ):
        self.model = model
        self.optimizer = optimizer
        self.criterion = criterion
        self.device = device
        self.grad_clip = grad_clip
        
        self.train_losses = []
        self.val_losses = []
        self.best_val_loss = float('inf')
        self.patience_counter = 0
    
    def train_epoch(self, dataloader: DataLoader, teacher_forcing_ratio: float = 0.5) -> float:
        """–û–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–µ"""
        self.model.train()
        epoch_loss = 0
        
        for batch_idx, (questions, answers, q_lengths, a_lengths) in enumerate(dataloader):
            questions = questions.to(self.device)
            answers = answers.to(self.device)
            q_lengths = q_lengths.to(self.device)
            
            self.optimizer.zero_grad()
            outputs = self.model(questions, answers, q_lengths, teacher_forcing_ratio)
            
            output_dim = outputs.shape[-1]
            outputs = outputs[:, 1:].reshape(-1, output_dim)
            answers = answers[:, 1:].reshape(-1)
            
            loss = self.criterion(outputs, answers)
            loss.backward()
            
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip)
            self.optimizer.step()
            
            epoch_loss += loss.item()
            
            if (batch_idx + 1) % TrainingConfig.LOG_EVERY == 0:
                avg_loss = epoch_loss / (batch_idx + 1)
                print(f"   Batch {batch_idx + 1}/{len(dataloader)} | Loss: {avg_loss:.4f}")
        
        return epoch_loss / len(dataloader)
    
    def validate(self, dataloader: DataLoader) -> float:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏"""
        self.model.eval()
        epoch_loss = 0
        
        with torch.no_grad():
            for questions, answers, q_lengths, a_lengths in dataloader:
                questions = questions.to(self.device)
                answers = answers.to(self.device)
                q_lengths = q_lengths.to(self.device)
                
                outputs = self.model(questions, answers, q_lengths, teacher_forcing_ratio=0.0)
                
                output_dim = outputs.shape[-1]
                outputs = outputs[:, 1:].reshape(-1, output_dim)
                answers = answers[:, 1:].reshape(-1)
                
                loss = self.criterion(outputs, answers)
                epoch_loss += loss.item()
        
        return epoch_loss / len(dataloader)
    
    def train(
        self,
        train_loader: DataLoader,
        val_loader: Optional[DataLoader] = None,
        num_epochs: int = 10,
        teacher_forcing_ratio: float = 0.5,
        save_dir: str = None,
        early_stopping_patience: int = 3
    ):
        """–ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è"""
        print("\n" + "=" * 60)
        print("–ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø")
        print("=" * 60)
        print(f"–≠–ø–æ—Ö: {num_epochs}")
        print(f"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        print(f"–ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {self.model.count_parameters():,}")
        print("=" * 60 + "\n")
        
        for epoch in range(num_epochs):
            start_time = time.time()
            
            print(f"\n–≠–ø–æ—Ö–∞ {epoch + 1}/{num_epochs}")
            print("-" * 60)
            
            train_loss = self.train_epoch(train_loader, teacher_forcing_ratio)
            self.train_losses.append(train_loss)
            
            if val_loader:
                val_loss = self.validate(val_loader)
                self.val_losses.append(val_loss)
                
                print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
                print(f"   Train Loss: {train_loss:.4f}")
                print(f"   Val Loss: {val_loss:.4f}")
                
                if val_loss < self.best_val_loss - TrainingConfig.MIN_DELTA:
                    self.best_val_loss = val_loss
                    self.patience_counter = 0
                    
                    if save_dir:
                        self.save_checkpoint(save_dir, epoch, train_loss, val_loss, is_best=True)
                    print(f"   ‚úÖ –ù–æ–≤–∞—è –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å! Val Loss: {val_loss:.4f}")
                else:
                    self.patience_counter += 1
                    print(f"   ‚ö†Ô∏è –ù–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è ({self.patience_counter}/{early_stopping_patience})")
                
                if self.patience_counter >= early_stopping_patience:
                    print(f"\n‚õî Early stopping!")
                    break
            else:
                print(f"\nüìä Train Loss: {train_loss:.4f}")
                
                if save_dir and (epoch + 1) % TrainingConfig.SAVE_EVERY == 0:
                    self.save_checkpoint(save_dir, epoch, train_loss, None)
            
            teacher_forcing_ratio *= 0.95
            
            epoch_time = time.time() - start_time
            print(f"   ‚è±Ô∏è –í—Ä–µ–º—è: {epoch_time:.2f} —Å–µ–∫")
        
        print("\n" + "=" * 60)
        print("–û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û")
        print("=" * 60)
        print(f"–õ—É—á—à–∏–π Val Loss: {self.best_val_loss:.4f}")
        print("=" * 60 + "\n")
    
    def save_checkpoint(
        self,
        save_dir: str,
        epoch: int,
        train_loss: float,
        val_loss: Optional[float] = None,
        is_best: bool = False
    ):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞ –º–æ–¥–µ–ª–∏
        
        Args:
            save_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            epoch: –ù–æ–º–µ—Ä —ç–ø–æ—Ö–∏
            train_loss: Train loss
            val_loss: Validation loss
            is_best: –õ—É—á—à–∞—è –ª–∏ —ç—Ç–æ –º–æ–¥–µ–ª—å
        """
        os.makedirs(save_dir, exist_ok=True)
        
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'train_loss': train_loss,
            'val_loss': val_loss,
            'train_losses': self.train_losses,
            'val_losses': self.val_losses
        }
        
        if is_best:
            path = os.path.join(save_dir, 'best_model.pt')
        else:
            path = os.path.join(save_dir, f'checkpoint_epoch_{epoch+1}.pt')
        
        torch.save(checkpoint, path)
        print(f"   üíæ –ß–µ–∫–ø–æ–∏–Ω—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {path}")
    
    def load_checkpoint(self, checkpoint_path: str):
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ —á–µ–∫–ø–æ–∏–Ω—Ç–∞
        
        Args:
            checkpoint_path: –ü—É—Ç—å –∫ —á–µ–∫–ø–æ–∏–Ω—Ç—É
        """
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.train_losses = checkpoint.get('train_losses', [])
        self.val_losses = checkpoint.get('val_losses', [])
        
        epoch = checkpoint['epoch']
        train_loss = checkpoint['train_loss']
        val_loss = checkpoint.get('val_loss')
        
        print(f"‚úÖ –ß–µ–∫–ø–æ–∏–Ω—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: {checkpoint_path}")
        print(f"   –≠–ø–æ—Ö–∞: {epoch + 1}")
        print(f"   Train Loss: {train_loss:.4f}")
        if val_loss:
            print(f"   Val Loss: {val_loss:.4f}")


def create_trainer(model: Seq2Seq, learning_rate: float = 0.001, device: str = 'cpu') -> Trainer:
    """–§–∞–±—Ä–∏—á–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è Trainer"""
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    criterion = nn.CrossEntropyLoss(ignore_index=0)
    
    trainer = Trainer(
        model=model,
        optimizer=optimizer,
        criterion=criterion,
        device=device,
        grad_clip=ModelConfig.GRAD_CLIP
    )
    
    return trainer


if __name__ == "__main__":
    """
    –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ save/load checkpoint
    """
    print("\n" + "=" * 60)
    print("–¢–ï–°–¢ TRAINER - –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫–∞")
    print("=" * 60)
    
    print("‚úÖ –ú–µ—Ç–æ–¥—ã –¥–æ–±–∞–≤–ª–µ–Ω—ã:")
    print("   - save_checkpoint(): —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏")
    print("   - load_checkpoint(): –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏")
    print("\nüíæ –ß—Ç–æ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è:")
    print("   - –í–µ—Å–∞ –º–æ–¥–µ–ª–∏ (model_state_dict)")
    print("   - –°–æ—Å—Ç–æ—è–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞")
    print("   - –ù–æ–º–µ—Ä —ç–ø–æ—Ö–∏")
    print("   - Train/Val losses")
    print("   - –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è")
    
    print("\n" + "=" * 60)
    print("‚úÖ TRAINER –ü–û–õ–ù–û–°–¢–¨–Æ –ì–û–¢–û–í")
    print("=" * 60)
