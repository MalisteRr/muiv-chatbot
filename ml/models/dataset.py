"""
DataLoader –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –±–∞—Ç—á–µ–π –¥–∞–Ω–Ω—ã—Ö

–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è Seq2Seq –º–æ–¥–µ–ª–∏:
1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ (–ø–∞—Ä—ã –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç)
2. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤
3. –°–æ–∑–¥–∞–Ω–∏–µ –±–∞—Ç—á–µ–π
4. –ü–∞–¥–¥–∏–Ω–≥ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
"""

import json
import torch
from torch.utils.data import Dataset
from typing import List, Dict, Tuple

from .tokenizer import SimpleTokenizer
from .config import ModelConfig


class QADataset(Dataset):
    """
    Dataset –¥–ª—è –ø–∞—Ä –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç
    """
    
    def __init__(
        self,
        data_path: str,
        tokenizer: SimpleTokenizer,
        max_length: int = 100
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
        
        Args:
            data_path: –ü—É—Ç—å –∫ JSON —Ñ–∞–π–ª—É —Å –¥–∞–Ω–Ω—ã–º–∏
            tokenizer: –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        """
        self.tokenizer = tokenizer
        self.max_length = max_length
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        with open(data_path, 'r', encoding='utf-8') as f:
            self.data = json.load(f)
        
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.data)} –ø–∞—Ä –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç")
    
    def __len__(self):
        """–†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        return len(self.data)
    
    def __getitem__(self, idx):
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞
        
        Args:
            idx: –ò–Ω–¥–µ–∫—Å –ø—Ä–∏–º–µ—Ä–∞
        
        Returns:
            question_indices: –ó–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤–æ–ø—Ä–æ—Å
            answer_indices: –ó–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
            question_length: –†–µ–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –≤–æ–ø—Ä–æ—Å–∞
            answer_length: –†–µ–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞
        """
        item = self.data[idx]
        question = item['question']
        answer = item['answer']
        
        # –ö–æ–¥–∏—Ä—É–µ–º –≤–æ–ø—Ä–æ—Å –∏ –æ—Ç–≤–µ—Ç
        question_indices = self.tokenizer.encode(
            question,
            max_length=self.max_length,
            add_sos=False,  # SOS –Ω–µ –Ω—É–∂–µ–Ω –¥–ª—è encoder
            add_eos=True    # EOS –Ω—É–∂–µ–Ω
        )
        
        answer_indices = self.tokenizer.encode(
            answer,
            max_length=self.max_length,
            add_sos=True,   # SOS –Ω—É–∂–µ–Ω –¥–ª—è decoder
            add_eos=True    # EOS —Ç–æ–∂–µ –Ω—É–∂–µ–Ω
        )
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ –¥–ª–∏–Ω—ã (–¥–æ –ø–∞–¥–¥–∏–Ω–≥–∞)
        question_length = sum(1 for idx in question_indices if idx != 0)
        answer_length = sum(1 for idx in answer_indices if idx != 0)
        
        return (
            torch.LongTensor(question_indices),
            torch.LongTensor(answer_indices),
            question_length,
            answer_length
        )


if __name__ == "__main__":
    """
    –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ QADataset
    """
    print("\n" + "=" * 60)
    print("–¢–ï–°–¢ QADATASET")
    print("=" * 60)
    
    # –°–æ–∑–¥–∞—ë–º –ø—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç
    import tempfile
    
    test_data = [
        {
            "question": "–°–∫–æ–ª—å–∫–æ —Å—Ç–æ–∏—Ç –æ–±—É—á–µ–Ω–∏–µ?",
            "answer": "–°—Ç–æ–∏–º–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç 150000 —Ä—É–±–ª–µ–π –≤ –≥–æ–¥.",
            "category": "–°—Ç–æ–∏–º–æ—Å—Ç—å"
        },
        {
            "question": "–ö–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω—É–∂–Ω—ã?",
            "answer": "–ù–µ–æ–±—Ö–æ–¥–∏–º—ã –ø–∞—Å–ø–æ—Ä—Ç, –∞—Ç—Ç–µ—Å—Ç–∞—Ç –∏ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏.",
            "category": "–î–æ–∫—É–º–µ–Ω—Ç—ã"
        },
        {
            "question": "–ï—Å—Ç—å –ª–∏ –±—é–¥–∂–µ—Ç–Ω—ã–µ –º–µ—Å—Ç–∞?",
            "answer": "–î–∞, –¥–æ—Å—Ç—É–ø–Ω–æ 25 –±—é–¥–∂–µ—Ç–Ω—ã—Ö –º–µ—Å—Ç.",
            "category": "–ë—é–¥–∂–µ—Ç"
        }
    ] * 10
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
    with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False, encoding='utf-8') as f:
        json.dump(test_data, f, ensure_ascii=False)
        temp_path = f.name
    
    # –°–æ–∑–¥–∞—ë–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    tokenizer = SimpleTokenizer(vocab_size=1000)
    all_texts = []
    for item in test_data:
        all_texts.append(item['question'])
        all_texts.append(item['answer'])
    tokenizer.build_vocab(all_texts)
    
    # –°–æ–∑–¥–∞—ë–º –¥–∞—Ç–∞—Å–µ—Ç
    dataset = QADataset(
        data_path=temp_path,
        tokenizer=tokenizer,
        max_length=50
    )
    
    print(f"\nüìä Dataset —Å–æ–∑–¥–∞–Ω:")
    print(f"   –†–∞–∑–º–µ—Ä: {len(dataset)}")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–ª—É—á–µ–Ω–∏–µ –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞
    q, a, q_len, a_len = dataset[0]
    
    print(f"\nüß™ –ü–µ—Ä–≤—ã–π –ø—Ä–∏–º–µ—Ä:")
    print(f"   Question —Ñ–æ—Ä–º–∞: {q.shape}")
    print(f"   Answer —Ñ–æ—Ä–º–∞: {a.shape}")
    print(f"   Q length: {q_len}")
    print(f"   A length: {a_len}")
    
    # –î–µ–∫–æ–¥–∏—Ä—É–µ–º
    decoded_q = tokenizer.decode(q.tolist())
    decoded_a = tokenizer.decode(a.tolist())
    print(f"\nüìù –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–∏–º–µ—Ä:")
    print(f"   –í–æ–ø—Ä–æ—Å: {decoded_q}")
    print(f"   –û—Ç–≤–µ—Ç: {decoded_a}")
    
    # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
    import os
    os.remove(temp_path)
    
    print("\n" + "=" * 60)
    print("‚úÖ QADATASET –†–ê–ë–û–¢–ê–ï–¢")
    print("=" * 60)
