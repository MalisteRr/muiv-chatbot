"""
DataLoader –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –±–∞—Ç—á–µ–π –¥–∞–Ω–Ω—ã—Ö

–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è Seq2Seq –º–æ–¥–µ–ª–∏:
1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ (–ø–∞—Ä—ã –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç)
2. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤
3. –°–æ–∑–¥–∞–Ω–∏–µ –±–∞—Ç—á–µ–π
4. –ü–∞–¥–¥–∏–Ω–≥ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
"""

import json
import torch
from torch.utils.data import Dataset, DataLoader
from typing import List, Dict, Tuple

from .tokenizer import SimpleTokenizer
from .config import ModelConfig


class QADataset(Dataset):
    """
    Dataset –¥–ª—è –ø–∞—Ä –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç
    """
    
    def __init__(
        self,
        data_path: str,
        tokenizer: SimpleTokenizer,
        max_length: int = 100
    ):
        self.tokenizer = tokenizer
        self.max_length = max_length
        
        with open(data_path, 'r', encoding='utf-8') as f:
            self.data = json.load(f)
        
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.data)} –ø–∞—Ä –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        question = item['question']
        answer = item['answer']
        
        question_indices = self.tokenizer.encode(
            question,
            max_length=self.max_length,
            add_sos=False,
            add_eos=True
        )
        
        answer_indices = self.tokenizer.encode(
            answer,
            max_length=self.max_length,
            add_sos=True,
            add_eos=True
        )
        
        question_length = sum(1 for idx in question_indices if idx != 0)
        answer_length = sum(1 for idx in answer_indices if idx != 0)
        
        return (
            torch.LongTensor(question_indices),
            torch.LongTensor(answer_indices),
            question_length,
            answer_length
        )


def collate_fn(batch):
    """
    –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –ø—Ä–∏–º–µ—Ä–æ–≤ –≤ –±–∞—Ç—á
    
    Args:
        batch: –°–ø–∏—Å–æ–∫ –ø—Ä–∏–º–µ—Ä–æ–≤ (–≤–æ–ø—Ä–æ—Å, –æ—Ç–≤–µ—Ç, –¥–ª–∏–Ω—ã)
    
    Returns:
        questions: –ë–∞—Ç—á –≤–æ–ø—Ä–æ—Å–æ–≤ (batch_size, max_seq_len)
        answers: –ë–∞—Ç—á –æ—Ç–≤–µ—Ç–æ–≤ (batch_size, max_seq_len)
        question_lengths: –î–ª–∏–Ω—ã –≤–æ–ø—Ä–æ—Å–æ–≤ (batch_size,)
        answer_lengths: –î–ª–∏–Ω—ã –æ—Ç–≤–µ—Ç–æ–≤ (batch_size,)
    """
    # –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º –±–∞—Ç—á
    questions, answers, q_lengths, a_lengths = zip(*batch)
    
    # –°—Ç–µ–∫–∞–µ–º –≤ —Ç–µ–Ω–∑–æ—Ä—ã
    questions = torch.stack(questions)
    answers = torch.stack(answers)
    question_lengths = torch.LongTensor(q_lengths)
    answer_lengths = torch.LongTensor(a_lengths)
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é –¥–ª–∏–Ω—ã –≤–æ–ø—Ä–æ—Å–æ–≤ (—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ pack_padded_sequence)
    sorted_indices = question_lengths.argsort(descending=True)
    
    questions = questions[sorted_indices]
    answers = answers[sorted_indices]
    question_lengths = question_lengths[sorted_indices]
    answer_lengths = answer_lengths[sorted_indices]
    
    return questions, answers, question_lengths, answer_lengths


def create_dataloaders(
    train_path: str,
    val_path: str = None,
    tokenizer: SimpleTokenizer = None,
    batch_size: int = 32,
    max_length: int = 100,
    num_workers: int = 0
) -> Tuple[DataLoader, DataLoader]:
    """
    –°–æ–∑–¥–∞–Ω–∏–µ DataLoader'–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    
    Args:
        train_path: –ü—É—Ç—å –∫ –æ–±—É—á–∞—é—â–∏–º –¥–∞–Ω–Ω—ã–º
        val_path: –ü—É—Ç—å –∫ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        tokenizer: –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
        max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        num_workers: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö
    
    Returns:
        train_loader: DataLoader –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        val_loader: DataLoader –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (–∏–ª–∏ None)
    """
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω
    if tokenizer is None:
        tokenizer = SimpleTokenizer.load(ModelConfig.TOKENIZER_PATH)
    
    # –°–æ–∑–¥–∞—ë–º –æ–±—É—á–∞—é—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç
    train_dataset = QADataset(
        data_path=train_path,
        tokenizer=tokenizer,
        max_length=max_length
    )
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        collate_fn=collate_fn,
        num_workers=num_workers,
        pin_memory=True  # –£—Å–∫–æ—Ä—è–µ—Ç –ø–µ—Ä–µ–¥–∞—á—É –Ω–∞ GPU
    )
    
    # –°–æ–∑–¥–∞—ë–º –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –µ—Å–ª–∏ –µ—Å—Ç—å
    val_loader = None
    if val_path:
        val_dataset = QADataset(
            data_path=val_path,
            tokenizer=tokenizer,
            max_length=max_length
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=batch_size,
            shuffle=False,
            collate_fn=collate_fn,
            num_workers=num_workers,
            pin_memory=True
        )
    
    print(f"\nüìä DataLoaders —Å–æ–∑–¥–∞–Ω—ã:")
    print(f"   Train –±–∞—Ç—á–µ–π: {len(train_loader)}")
    print(f"   Val –±–∞—Ç—á–µ–π: {len(val_loader) if val_loader else 0}")
    print(f"   Batch size: {batch_size}")
    
    return train_loader, val_loader


if __name__ == "__main__":
    """
    –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ DataLoader
    """
    print("\n" + "=" * 60)
    print("–¢–ï–°–¢ DATALOADER")
    print("=" * 60)
    
    import tempfile
    
    test_data = [
        {
            "question": "–°–∫–æ–ª—å–∫–æ —Å—Ç–æ–∏—Ç –æ–±—É—á–µ–Ω–∏–µ?",
            "answer": "–°—Ç–æ–∏–º–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç 150000 —Ä—É–±–ª–µ–π –≤ –≥–æ–¥.",
            "category": "–°—Ç–æ–∏–º–æ—Å—Ç—å"
        },
        {
            "question": "–ö–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω—É–∂–Ω—ã?",
            "answer": "–ù–µ–æ–±—Ö–æ–¥–∏–º—ã –ø–∞—Å–ø–æ—Ä—Ç, –∞—Ç—Ç–µ—Å—Ç–∞—Ç –∏ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏.",
            "category": "–î–æ–∫—É–º–µ–Ω—Ç—ã"
        }
    ] * 10
    
    with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False, encoding='utf-8') as f:
        json.dump(test_data, f, ensure_ascii=False)
        temp_path = f.name
    
    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    tokenizer = SimpleTokenizer(vocab_size=1000)
    all_texts = []
    for item in test_data:
        all_texts.append(item['question'])
        all_texts.append(item['answer'])
    tokenizer.build_vocab(all_texts)
    
    # –î–∞—Ç–∞—Å–µ—Ç
    dataset = QADataset(temp_path, tokenizer, max_length=50)
    
    # DataLoader
    dataloader = DataLoader(
        dataset,
        batch_size=4,
        shuffle=True,
        collate_fn=collate_fn
    )
    
    print(f"\nüìä DataLoader —Å–æ–∑–¥–∞–Ω:")
    print(f"   –ë–∞—Ç—á–µ–π: {len(dataloader)}")
    
    # –¢–µ—Å—Ç –±–∞—Ç—á–∞
    questions, answers, q_lengths, a_lengths = next(iter(dataloader))
    
    print(f"\nüß™ –¢–µ—Å—Ç–æ–≤—ã–π –±–∞—Ç—á:")
    print(f"   Questions: {questions.shape}")
    print(f"   Answers: {answers.shape}")
    print(f"   Q lengths: {q_lengths.tolist()}")
    print(f"   A lengths: {a_lengths.tolist()}")
    
    import os
    os.remove(temp_path)
    
    print("\n" + "=" * 60)
    print("‚úÖ DATALOADER –†–ê–ë–û–¢–ê–ï–¢")
    print("=" * 60)
