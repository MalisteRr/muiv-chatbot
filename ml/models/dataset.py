"""
DataLoader –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –±–∞—Ç—á–µ–π –¥–∞–Ω–Ω—ã—Ö

–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è Seq2Seq –º–æ–¥–µ–ª–∏:
1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ (–ø–∞—Ä—ã –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç)
2. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤
3. –°–æ–∑–¥–∞–Ω–∏–µ –±–∞—Ç—á–µ–π
4. –ü–∞–¥–¥–∏–Ω–≥ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
"""

import json
import torch
from torch.utils.data import Dataset, DataLoader
from typing import List, Dict, Tuple

from .tokenizer import SimpleTokenizer
from .config import ModelConfig


class QADataset(Dataset):
    """Dataset –¥–ª—è –ø–∞—Ä –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç"""
    
    def __init__(self, data_path: str, tokenizer: SimpleTokenizer, max_length: int = 100):
        self.tokenizer = tokenizer
        self.max_length = max_length
        
        with open(data_path, 'r', encoding='utf-8') as f:
            self.data = json.load(f)
        
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.data)} –ø–∞—Ä –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        question = item['question']
        answer = item['answer']
        
        question_indices = self.tokenizer.encode(
            question, max_length=self.max_length, add_sos=False, add_eos=True
        )
        answer_indices = self.tokenizer.encode(
            answer, max_length=self.max_length, add_sos=True, add_eos=True
        )
        
        question_length = sum(1 for idx in question_indices if idx != 0)
        answer_length = sum(1 for idx in answer_indices if idx != 0)
        
        return (
            torch.LongTensor(question_indices),
            torch.LongTensor(answer_indices),
            question_length,
            answer_length
        )


def collate_fn(batch):
    """–§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –ø—Ä–∏–º–µ—Ä–æ–≤ –≤ –±–∞—Ç—á"""
    questions, answers, q_lengths, a_lengths = zip(*batch)
    
    questions = torch.stack(questions)
    answers = torch.stack(answers)
    question_lengths = torch.LongTensor(q_lengths)
    answer_lengths = torch.LongTensor(a_lengths)
    
    sorted_indices = question_lengths.argsort(descending=True)
    
    questions = questions[sorted_indices]
    answers = answers[sorted_indices]
    question_lengths = question_lengths[sorted_indices]
    answer_lengths = answer_lengths[sorted_indices]
    
    return questions, answers, question_lengths, answer_lengths


def create_dataloaders(
    train_path: str,
    val_path: str = None,
    tokenizer: SimpleTokenizer = None,
    batch_size: int = 32,
    max_length: int = 100,
    num_workers: int = 0
) -> Tuple[DataLoader, DataLoader]:
    """–°–æ–∑–¥–∞–Ω–∏–µ DataLoader'–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏"""
    if tokenizer is None:
        tokenizer = SimpleTokenizer.load(ModelConfig.TOKENIZER_PATH)
    
    train_dataset = QADataset(train_path, tokenizer, max_length)
    train_loader = DataLoader(
        train_dataset, batch_size=batch_size, shuffle=True,
        collate_fn=collate_fn, num_workers=num_workers, pin_memory=True
    )
    
    val_loader = None
    if val_path:
        val_dataset = QADataset(val_path, tokenizer, max_length)
        val_loader = DataLoader(
            val_dataset, batch_size=batch_size, shuffle=False,
            collate_fn=collate_fn, num_workers=num_workers, pin_memory=True
        )
    
    print(f"\nüìä DataLoaders —Å–æ–∑–¥–∞–Ω—ã:")
    print(f"   Train –±–∞—Ç—á–µ–π: {len(train_loader)}")
    print(f"   Val –±–∞—Ç—á–µ–π: {len(val_loader) if val_loader else 0}")
    print(f"   Batch size: {batch_size}")
    
    return train_loader, val_loader


def split_dataset(
    data_path: str,
    train_ratio: float = 0.8,
    val_ratio: float = 0.1,
    test_ratio: float = 0.1,
    save_splits: bool = True
) -> Tuple[str, str, str]:
    """
    –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–∞ train/val/test
    
    Args:
        data_path: –ü—É—Ç—å –∫ –ø–æ–ª–Ω–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É
        train_ratio: –î–æ–ª—è –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        val_ratio: –î–æ–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        test_ratio: –î–æ–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        save_splits: –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –ª–∏ —Ä–∞–∑–¥–µ–ª—ë–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    
    Returns:
        train_path: –ü—É—Ç—å –∫ –æ–±—É—á–∞—é—â–∏–º –¥–∞–Ω–Ω—ã–º
        val_path: –ü—É—Ç—å –∫ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º
        test_path: –ü—É—Ç—å –∫ —Ç–µ—Å—Ç–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º
    """
    import os
    import random
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–π
    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, \
        "–°—É–º–º–∞ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–π –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å 1.0"
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    with open(data_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º
    random.shuffle(data)
    
    # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–º–µ—Ä—ã
    total = len(data)
    train_size = int(total * train_ratio)
    val_size = int(total * val_ratio)
    
    # –†–∞–∑–¥–µ–ª—è–µ–º
    train_data = data[:train_size]
    val_data = data[train_size:train_size + val_size]
    test_data = data[train_size + val_size:]
    
    print(f"\n‚úÇÔ∏è –î–∞—Ç–∞—Å–µ—Ç —Ä–∞–∑–¥–µ–ª—ë–Ω:")
    print(f"   Train: {len(train_data)} ({len(train_data)/total*100:.1f}%)")
    print(f"   Val: {len(val_data)} ({len(val_data)/total*100:.1f}%)")
    print(f"   Test: {len(test_data)} ({len(test_data)/total*100:.1f}%)")
    
    if save_splits:
        # –ü—É—Ç–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        base_dir = os.path.dirname(data_path)
        train_path = os.path.join(base_dir, 'train_data.json')
        val_path = os.path.join(base_dir, 'val_data.json')
        test_path = os.path.join(base_dir, 'test_data.json')
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        with open(train_path, 'w', encoding='utf-8') as f:
            json.dump(train_data, f, ensure_ascii=False, indent=2)
        
        with open(val_path, 'w', encoding='utf-8') as f:
            json.dump(val_data, f, ensure_ascii=False, indent=2)
        
        with open(test_path, 'w', encoding='utf-8') as f:
            json.dump(test_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nüíæ –§–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
        print(f"   {train_path}")
        print(f"   {val_path}")
        print(f"   {test_path}")
        
        return train_path, val_path, test_path
    
    return None, None, None


if __name__ == "__main__":
    """
    –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ split_dataset
    """
    print("\n" + "=" * 60)
    print("–¢–ï–°–¢ SPLIT_DATASET")
    print("=" * 60)
    
    import tempfile
    
    # –°–æ–∑–¥–∞—ë–º —Ç–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç
    test_data = [
        {"question": f"–í–æ–ø—Ä–æ—Å {i}", "answer": f"–û—Ç–≤–µ—Ç {i}", "category": "–¢–µ—Å—Ç"}
        for i in range(100)
    ]
    
    with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False, encoding='utf-8') as f:
        json.dump(test_data, f, ensure_ascii=False)
        temp_path = f.name
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞—Ç–∞—Å–µ—Ç
    train_path, val_path, test_path = split_dataset(
        temp_path,
        train_ratio=0.8,
        val_ratio=0.1,
        test_ratio=0.1,
        save_splits=True
    )
    
    print(f"\n‚úÖ –î–∞—Ç–∞—Å–µ—Ç —É—Å–ø–µ—à–Ω–æ —Ä–∞–∑–¥–µ–ª—ë–Ω!")
    
    # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
    import os
    os.remove(temp_path)
    os.remove(train_path)
    os.remove(val_path)
    os.remove(test_path)
    
    print("\n" + "=" * 60)
    print("‚úÖ DATASET –ü–û–õ–ù–û–°–¢–¨–Æ –ì–û–¢–û–í")
    print("=" * 60)
