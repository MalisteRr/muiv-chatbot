"""
Encoder (–ö–æ–¥–∏—Ä–æ–≤—â–∏–∫) –Ω–∞ –æ—Å–Ω–æ–≤–µ LSTM
–ê–≤—Ç–æ—Ä: –°–∏–Ω–∏—Ü–∏–Ω –ú–∏—Ö–∞–∏–ª
–¢–µ–º–∞ –í–ö–†: –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ —á–∞—Ç-–±–æ—Ç–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–π –∞–±–∏—Ç—É—Ä–∏–µ–Ω—Ç–æ–≤

Encoder –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –≤—Ö–æ–¥–Ω—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å (–≤–æ–ø—Ä–æ—Å) –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –≤–µ–∫—Ç–æ—Ä,
–∫–æ—Ç–æ—Ä—ã–π –∑–∞—Ç–µ–º –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Decoder'–æ–º –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞.

–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
Input ‚Üí Embedding ‚Üí LSTM ‚Üí Hidden State (–∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –≤–µ–∫—Ç–æ—Ä)
"""

import torch
import torch.nn as nn


class Encoder(nn.Module):
    """
    Encoder –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω–æ–≥–æ LSTM
    
    –ü—Ä–∏–Ω–∏–º–∞–µ—Ç –Ω–∞ –≤—Ö–æ–¥ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–Ω–¥–µ–∫—Å–æ–≤ —Å–ª–æ–≤,
    –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ (hidden state) –∏ —Å–æ—Å—Ç–æ—è–Ω–∏–µ —è—á–µ–π–∫–∏ (cell state)
    """
    
    def __init__(
        self, 
        vocab_size: int,
        embedding_dim: int = 256,
        hidden_size: int = 512,
        num_layers: int = 2,
        dropout: float = 0.3
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Encoder
        
        Args:
            vocab_size: –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è
            embedding_dim: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            hidden_size: –†–∞–∑–º–µ—Ä —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è LSTM
            num_layers: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ—ë–≤ LSTM
            dropout: –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å dropout (—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è)
        """
        super(Encoder, self).__init__()
        
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.dropout = dropout
        
        # –°–ª–æ–π —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä—ã)
        self.embedding = nn.Embedding(
            num_embeddings=vocab_size,
            embedding_dim=embedding_dim,
            padding_idx=0  # –ò–Ω–¥–µ–∫—Å PAD —Ç–æ–∫–µ–Ω–∞
        )
        
        # LSTM —Å–ª–æ–π (–º–æ–∂–µ—Ç –±—ã—Ç—å –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã–º)
        self.lstm = nn.LSTM(
            input_size=embedding_dim,
            hidden_size=hidden_size,
            num_layers=num_layers,
            dropout=dropout if num_layers > 1 else 0,  # Dropout –º–µ–∂–¥—É —Å–ª–æ—è–º–∏
            batch_first=True  # –§–æ—Ä–º–∞—Ç: (batch, seq_len, features)
        )
        
        # Dropout –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        self.dropout_layer = nn.Dropout(dropout)
    
    def forward(self, input_seq, input_lengths=None):
        """
        –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ Encoder
        
        Args:
            input_seq: –í—Ö–æ–¥–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–Ω–¥–µ–∫—Å–æ–≤
                       –§–æ—Ä–º–∞: (batch_size, seq_length)
            input_lengths: –†–µ–∞–ª—å–Ω—ã–µ –¥–ª–∏–Ω—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
                          –§–æ—Ä–º–∞: (batch_size,)
        
        Returns:
            outputs: –í—ã—Ö–æ–¥—ã LSTM –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —à–∞–≥–∞
                    –§–æ—Ä–º–∞: (batch_size, seq_length, hidden_size)
            hidden: –°–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–ª–æ—è
                   –§–æ—Ä–º–∞: (num_layers, batch_size, hidden_size)
            cell: –°–æ—Å—Ç–æ—è–Ω–∏–µ —è—á–µ–π–∫–∏ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–ª–æ—è
                 –§–æ—Ä–º–∞: (num_layers, batch_size, hidden_size)
        """
        # 1. –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∏–Ω–¥–µ–∫—Å—ã –≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        # input_seq: (batch_size, seq_length)
        # embedded: (batch_size, seq_length, embedding_dim)
        embedded = self.embedding(input_seq)
        
        # 2. –ü—Ä–∏–º–µ–Ω—è–µ–º dropout –∫ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º
        embedded = self.dropout_layer(embedded)
        
        # 3. –ï—Å–ª–∏ –µ—Å—Ç—å —Ä–µ–∞–ª—å–Ω—ã–µ –¥–ª–∏–Ω—ã - –∏—Å–ø–æ–ª—å–∑—É–µ–º pack_padded_sequence
        # –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç LSTM –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–∞–¥–¥–∏–Ω–≥
        if input_lengths is not None:
            # –£–ø–∞–∫–æ–≤—ã–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            packed = nn.utils.rnn.pack_padded_sequence(
                embedded, 
                input_lengths.cpu(), 
                batch_first=True, 
                enforce_sorted=False
            )
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —á–µ—Ä–µ–∑ LSTM
            packed_outputs, (hidden, cell) = self.lstm(packed)
            
            # –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º –æ–±—Ä–∞—Ç–Ω–æ
            outputs, _ = nn.utils.rnn.pad_packed_sequence(
                packed_outputs, 
                batch_first=True
            )
        else:
            # –û–±—ã—á–Ω—ã–π –ø—Ä–æ—Ö–æ–¥ –±–µ–∑ —É–ø–∞–∫–æ–≤–∫–∏
            outputs, (hidden, cell) = self.lstm(embedded)
        
        # outputs: (batch_size, seq_length, hidden_size)
        # hidden: (num_layers, batch_size, hidden_size)
        # cell: (num_layers, batch_size, hidden_size)
        
        return outputs, hidden, cell


if __name__ == "__main__":
    """
    –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Encoder —Å –º–µ—Ç–æ–¥–æ–º forward
    """
    print("\n" + "=" * 60)
    print("–¢–ï–°–¢ ENCODER - –ú–µ—Ç–æ–¥ forward")
    print("=" * 60)
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
    vocab_size = 5000
    embedding_dim = 256
    hidden_size = 512
    num_layers = 2
    batch_size = 4
    seq_length = 20
    
    # –°–æ–∑–¥–∞—ë–º Encoder
    encoder = Encoder(
        vocab_size=vocab_size,
        embedding_dim=embedding_dim,
        hidden_size=hidden_size,
        num_layers=num_layers,
        dropout=0.3
    )
    
    print(f"‚úÖ Encoder —Å–æ–∑–¥–∞–Ω:")
    print(f"   Vocab size: {vocab_size}")
    print(f"   Embedding dim: {embedding_dim}")
    print(f"   Hidden size: {hidden_size}")
    print(f"   Num layers: {num_layers}")
    print(f"\nüìä –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ –º–æ–¥–µ–ª–∏: {sum(p.numel() for p in encoder.parameters()):,}")
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    test_input = torch.randint(0, vocab_size, (batch_size, seq_length))
    test_lengths = torch.tensor([20, 18, 15, 12])
    
    print(f"\nüß™ –¢–µ—Å—Ç–æ–≤—ã–π –≤—Ö–æ–¥:")
    print(f"   –§–æ—Ä–º–∞: {test_input.shape}")
    print(f"   –î–ª–∏–Ω—ã: {test_lengths.tolist()}")
    
    # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
    encoder.eval()
    with torch.no_grad():
        outputs, hidden, cell = encoder(test_input, test_lengths)
    
    print(f"\nüì§ –í—ã—Ö–æ–¥ Encoder:")
    print(f"   Outputs —Ñ–æ—Ä–º–∞: {outputs.shape}")
    print(f"   Hidden —Ñ–æ—Ä–º–∞: {hidden.shape}")
    print(f"   Cell —Ñ–æ—Ä–º–∞: {cell.shape}")
    
    print("\n" + "=" * 60)
    print("‚úÖ –ú–ï–¢–û–î FORWARD –†–ê–ë–û–¢–ê–ï–¢")
    print("=" * 60)
