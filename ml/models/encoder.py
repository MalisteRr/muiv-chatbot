"""
Encoder (–ö–æ–¥–∏—Ä–æ–≤—â–∏–∫) –Ω–∞ –æ—Å–Ω–æ–≤–µ LSTM
–ê–≤—Ç–æ—Ä: –°–∏–Ω–∏—Ü–∏–Ω –ú–∏—Ö–∞–∏–ª
–¢–µ–º–∞ –í–ö–†: –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ —á–∞—Ç-–±–æ—Ç–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–π –∞–±–∏—Ç—É—Ä–∏–µ–Ω—Ç–æ–≤

Encoder –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –≤—Ö–æ–¥–Ω—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å (–≤–æ–ø—Ä–æ—Å) –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –≤–µ–∫—Ç–æ—Ä,
–∫–æ—Ç–æ—Ä—ã–π –∑–∞—Ç–µ–º –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Decoder'–æ–º –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞.

–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
Input ‚Üí Embedding ‚Üí LSTM ‚Üí Hidden State (–∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –≤–µ–∫—Ç–æ—Ä)
"""

import torch
import torch.nn as nn


class Encoder(nn.Module):
    """
    Encoder –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω–æ–≥–æ LSTM
    
    –ü—Ä–∏–Ω–∏–º–∞–µ—Ç –Ω–∞ –≤—Ö–æ–¥ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–Ω–¥–µ–∫—Å–æ–≤ —Å–ª–æ–≤,
    –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ (hidden state) –∏ —Å–æ—Å—Ç–æ—è–Ω–∏–µ —è—á–µ–π–∫–∏ (cell state)
    """
    
    def __init__(
        self, 
        vocab_size: int,
        embedding_dim: int = 256,
        hidden_size: int = 512,
        num_layers: int = 2,
        dropout: float = 0.3
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Encoder
        
        Args:
            vocab_size: –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è
            embedding_dim: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            hidden_size: –†–∞–∑–º–µ—Ä —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è LSTM
            num_layers: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ—ë–≤ LSTM
            dropout: –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å dropout (—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è)
        """
        super(Encoder, self).__init__()
        
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.dropout = dropout
        
        # –°–ª–æ–π —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä—ã)
        self.embedding = nn.Embedding(
            num_embeddings=vocab_size,
            embedding_dim=embedding_dim,
            padding_idx=0  # –ò–Ω–¥–µ–∫—Å PAD —Ç–æ–∫–µ–Ω–∞
        )
        
        # LSTM —Å–ª–æ–π (–º–æ–∂–µ—Ç –±—ã—Ç—å –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã–º)
        self.lstm = nn.LSTM(
            input_size=embedding_dim,
            hidden_size=hidden_size,
            num_layers=num_layers,
            dropout=dropout if num_layers > 1 else 0,  # Dropout –º–µ–∂–¥—É —Å–ª–æ—è–º–∏
            batch_first=True  # –§–æ—Ä–º–∞—Ç: (batch, seq_len, features)
        )
        
        # Dropout –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        self.dropout_layer = nn.Dropout(dropout)


if __name__ == "__main__":
    """
    –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –±–∞–∑–æ–≤–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã Encoder
    """
    print("\n" + "=" * 60)
    print("–¢–ï–°–¢ ENCODER - –ë–∞–∑–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞")
    print("=" * 60)
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
    vocab_size = 5000
    embedding_dim = 256
    hidden_size = 512
    num_layers = 2
    
    # –°–æ–∑–¥–∞—ë–º Encoder
    encoder = Encoder(
        vocab_size=vocab_size,
        embedding_dim=embedding_dim,
        hidden_size=hidden_size,
        num_layers=num_layers,
        dropout=0.3
    )
    
    print(f"‚úÖ Encoder —Å–æ–∑–¥–∞–Ω:")
    print(f"   Vocab size: {vocab_size}")
    print(f"   Embedding dim: {embedding_dim}")
    print(f"   Hidden size: {hidden_size}")
    print(f"   Num layers: {num_layers}")
    print(f"\nüìä –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ –º–æ–¥–µ–ª–∏: {sum(p.numel() for p in encoder.parameters()):,}")
    
    print("\n" + "=" * 60)
    print("‚úÖ –ë–ê–ó–û–í–ê–Ø –°–¢–†–£–ö–¢–£–†–ê –ì–û–¢–û–í–ê")
    print("=" * 60)
