"""
Decoder (–î–µ–∫–æ–¥–µ—Ä) –Ω–∞ –æ—Å–Ω–æ–≤–µ LSTM —Å –º–µ—Ö–∞–Ω–∏–∑–º–æ–º –≤–Ω–∏–º–∞–Ω–∏—è (Attention)

Decoder –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ –æ—Ç Encoder'–∞.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è (Attention) –¥–ª—è —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∫–∏ –Ω–∞ –≤–∞–∂–Ω—ã—Ö —á–∞—Å—Ç—è—Ö –≤—Ö–æ–¥–∞.

–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
Context Vector ‚Üí LSTM ‚Üí Attention ‚Üí Linear ‚Üí Output (—Å–ª–µ–¥—É—é—â–µ–µ —Å–ª–æ–≤–æ)
"""

import torch
import torch.nn as nn
import torch.nn.functional as F


class Attention(nn.Module):
    """
    –ú–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è (Bahdanau Attention)
    """
    
    def __init__(self, hidden_size: int):
        super(Attention, self).__init__()
        
        self.hidden_size = hidden_size
        self.attn = nn.Linear(hidden_size * 2, hidden_size)
        self.v = nn.Linear(hidden_size, 1, bias=False)
    
    def forward(self, hidden, encoder_outputs):
        batch_size = encoder_outputs.size(0)
        seq_length = encoder_outputs.size(1)
        
        hidden = hidden.unsqueeze(1).repeat(1, seq_length, 1)
        energy = torch.tanh(self.attn(torch.cat([hidden, encoder_outputs], dim=2)))
        attention = self.v(energy).squeeze(2)
        attention_weights = F.softmax(attention, dim=1)
        
        return attention_weights


class Decoder(nn.Module):
    """
    Decoder –Ω–∞ –æ—Å–Ω–æ–≤–µ LSTM —Å –º–µ—Ö–∞–Ω–∏–∑–º–æ–º –≤–Ω–∏–º–∞–Ω–∏—è
    """
    
    def __init__(
        self,
        vocab_size: int,
        embedding_dim: int = 256,
        hidden_size: int = 512,
        num_layers: int = 2,
        dropout: float = 0.3,
        use_attention: bool = True
    ):
        super(Decoder, self).__init__()
        
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.dropout = dropout
        self.use_attention = use_attention
        
        # –°–ª–æ–π —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        self.embedding = nn.Embedding(
            num_embeddings=vocab_size,
            embedding_dim=embedding_dim,
            padding_idx=0
        )
        
        # LSTM —Å–ª–æ–π
        lstm_input_size = embedding_dim + hidden_size if use_attention else embedding_dim
        
        self.lstm = nn.LSTM(
            input_size=lstm_input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            dropout=dropout if num_layers > 1 else 0,
            batch_first=True
        )
        
        # –ú–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è
        if use_attention:
            self.attention = Attention(hidden_size)
        
        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
        fc_input_size = hidden_size * 2 if use_attention else hidden_size
        self.fc = nn.Linear(fc_input_size, vocab_size)
        
        # Dropout
        self.dropout_layer = nn.Dropout(dropout)
    
    def forward(self, input_token, hidden, cell, encoder_outputs=None):
        """
        –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ Decoder (–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–¥–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞)
        
        Args:
            input_token: –í—Ö–æ–¥–Ω–æ–π —Ç–æ–∫–µ–Ω (–ø—Ä–µ–¥—ã–¥—É—â–µ–µ —Å–ª–æ–≤–æ)
                        –§–æ—Ä–º–∞: (batch_size, 1)
            hidden: –°–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
                   –§–æ—Ä–º–∞: (num_layers, batch_size, hidden_size)
            cell: –°–æ—Å—Ç–æ—è–Ω–∏–µ —è—á–µ–π–∫–∏
                 –§–æ—Ä–º–∞: (num_layers, batch_size, hidden_size)
            encoder_outputs: –í—ã—Ö–æ–¥—ã encoder'–∞ (–¥–ª—è attention)
                           –§–æ—Ä–º–∞: (batch_size, seq_length, hidden_size)
        
        Returns:
            output: –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–ª–æ–≤–∞
                   –§–æ—Ä–º–∞: (batch_size, vocab_size)
            hidden: –ù–æ–≤–æ–µ —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
            cell: –ù–æ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ —è—á–µ–π–∫–∏
            attention_weights: –í–µ—Å–∞ –≤–Ω–∏–º–∞–Ω–∏—è (–µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è)
        """
        # 1. –≠–º–±–µ–¥–¥–∏–Ω–≥ –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞
        embedded = self.embedding(input_token)
        embedded = self.dropout_layer(embedded)
        
        # 2. –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ —á–µ—Ä–µ–∑ attention
        attention_weights = None
        if self.use_attention and encoder_outputs is not None:
            # –ë–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–æ–π hidden –¥–ª—è attention
            last_hidden = hidden[-1]
            
            # –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Å–∞ –≤–Ω–∏–º–∞–Ω–∏—è
            attention_weights = self.attention(last_hidden, encoder_outputs)
            
            # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –≤–µ–∫—Ç–æ—Ä (–≤–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞ encoder outputs)
            context = torch.bmm(
                attention_weights.unsqueeze(1), 
                encoder_outputs
            )
            
            # –ö–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç
            lstm_input = torch.cat([embedded, context], dim=2)
        else:
            # –ë–µ–∑ attention
            lstm_input = embedded
        
        # 3. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —á–µ—Ä–µ–∑ LSTM
        output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))
        
        # 4. –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –≤—Ö–æ–¥ –¥–ª—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è
        if self.use_attention:
            # –ö–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É–µ–º LSTM output –∏ context
            fc_input = torch.cat([output, context], dim=2)
        else:
            fc_input = output
        
        # –£–±–∏—Ä–∞–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å seq_length (–æ–Ω–∞ = 1)
        fc_input = fc_input.squeeze(1)
        
        # 5. –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π (—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Å–ª–æ–≤–∞—Ä—é)
        prediction = self.fc(fc_input)
        
        return prediction, hidden, cell, attention_weights


if __name__ == "__main__":
    """
    –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Decoder —Å forward
    """
    print("\n" + "=" * 60)
    print("–¢–ï–°–¢ DECODER - –ú–µ—Ç–æ–¥ forward")
    print("=" * 60)
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
    vocab_size = 5000
    embedding_dim = 256
    hidden_size = 512
    num_layers = 2
    batch_size = 4
    seq_length = 20
    
    # –°–æ–∑–¥–∞—ë–º Decoder
    decoder = Decoder(
        vocab_size=vocab_size,
        embedding_dim=embedding_dim,
        hidden_size=hidden_size,
        num_layers=num_layers,
        dropout=0.3,
        use_attention=True
    )
    
    print(f"‚úÖ Decoder —Å–æ–∑–¥–∞–Ω —Å attention")
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    test_input = torch.randint(0, vocab_size, (batch_size, 1))
    test_hidden = torch.randn(num_layers, batch_size, hidden_size)
    test_cell = torch.randn(num_layers, batch_size, hidden_size)
    test_encoder_outputs = torch.randn(batch_size, seq_length, hidden_size)
    
    print(f"\nüß™ –¢–µ—Å—Ç–æ–≤—ã–π –≤—Ö–æ–¥:")
    print(f"   Input —Ñ–æ—Ä–º–∞: {test_input.shape}")
    print(f"   Encoder outputs —Ñ–æ—Ä–º–∞: {test_encoder_outputs.shape}")
    
    # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
    decoder.eval()
    with torch.no_grad():
        prediction, hidden, cell, attention_weights = decoder(
            test_input, 
            test_hidden, 
            test_cell, 
            test_encoder_outputs
        )
    
    print(f"\nüì§ –í—ã—Ö–æ–¥ Decoder:")
    print(f"   Prediction —Ñ–æ—Ä–º–∞: {prediction.shape}")
    print(f"   Hidden —Ñ–æ—Ä–º–∞: {hidden.shape}")
    print(f"   Attention weights —Ñ–æ—Ä–º–∞: {attention_weights.shape}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
    probs = F.softmax(prediction, dim=1)
    print(f"\nüìä –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏:")
    print(f"   –°—É–º–º–∞: {probs[0].sum().item():.4f} (–¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å ‚âà1.0)")
    
    print("\n" + "=" * 60)
    print("‚úÖ DECODER –° FORWARD –†–ê–ë–û–¢–ê–ï–¢")
    print("=" * 60)
