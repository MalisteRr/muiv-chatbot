"""
Decoder (–î–µ–∫–æ–¥–µ—Ä) –Ω–∞ –æ—Å–Ω–æ–≤–µ LSTM —Å –º–µ—Ö–∞–Ω–∏–∑–º–æ–º –≤–Ω–∏–º–∞–Ω–∏—è (Attention)

Decoder –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ –æ—Ç Encoder'–∞.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è (Attention) –¥–ª—è —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∫–∏ –Ω–∞ –≤–∞–∂–Ω—ã—Ö —á–∞—Å—Ç—è—Ö –≤—Ö–æ–¥–∞.

–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
Context Vector ‚Üí LSTM ‚Üí Attention ‚Üí Linear ‚Üí Output (—Å–ª–µ–¥—É—é—â–µ–µ —Å–ª–æ–≤–æ)
"""

import torch
import torch.nn as nn
import torch.nn.functional as F


class Attention(nn.Module):
    """
    –ú–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è (Bahdanau Attention)
    
    –ü–æ–∑–≤–æ–ª—è–µ—Ç decoder'—É "—Å–º–æ—Ç—Ä–µ—Ç—å" –Ω–∞ —Ä–∞–∑–Ω—ã–µ —á–∞—Å—Ç–∏ –≤—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ –æ—Ç–≤–µ—Ç–∞.
    """
    
    def __init__(self, hidden_size: int):
        """
        Args:
            hidden_size: –†–∞–∑–º–µ—Ä —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è
        """
        super(Attention, self).__init__()
        
        self.hidden_size = hidden_size
        
        # –õ–∏–Ω–µ–π–Ω—ã–µ —Å–ª–æ–∏ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤–µ—Å–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è
        self.attn = nn.Linear(hidden_size * 2, hidden_size)
        self.v = nn.Linear(hidden_size, 1, bias=False)
    
    def forward(self, hidden, encoder_outputs):
        """
        –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è
        
        Args:
            hidden: –°–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ decoder'–∞
                   –§–æ—Ä–º–∞: (batch_size, hidden_size)
            encoder_outputs: –í—ã—Ö–æ–¥—ã encoder'–∞ –¥–ª—è –≤—Å–µ—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —à–∞–≥–æ–≤
                           –§–æ—Ä–º–∞: (batch_size, seq_length, hidden_size)
        
        Returns:
            attention_weights: –í–µ—Å–∞ –≤–Ω–∏–º–∞–Ω–∏—è
                             –§–æ—Ä–º–∞: (batch_size, seq_length)
        """
        batch_size = encoder_outputs.size(0)
        seq_length = encoder_outputs.size(1)
        
        # –ü–æ–≤—Ç–æ—Ä—è–µ–º hidden –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —à–∞–≥–∞
        hidden = hidden.unsqueeze(1).repeat(1, seq_length, 1)
        
        # –ö–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É–µ–º hidden –∏ encoder_outputs
        energy = torch.tanh(self.attn(torch.cat([hidden, encoder_outputs], dim=2)))
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å–∫–æ—Ä—ã –≤–Ω–∏–º–∞–Ω–∏—è
        attention = self.v(energy).squeeze(2)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º softmax –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
        attention_weights = F.softmax(attention, dim=1)
        
        return attention_weights


class Decoder(nn.Module):
    """
    Decoder –Ω–∞ –æ—Å–Ω–æ–≤–µ LSTM —Å –º–µ—Ö–∞–Ω–∏–∑–º–æ–º –≤–Ω–∏–º–∞–Ω–∏—è
    
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –ø–æ –æ–¥–Ω–æ–º—É —Å–ª–æ–≤—É –∑–∞ —Ä–∞–∑,
    –∏—Å–ø–æ–ª—å–∑—É—è –∫–æ–Ω—Ç–µ–∫—Å—Ç –æ—Ç encoder'–∞ –∏ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–ª–æ–≤–∞.
    """
    
    def __init__(
        self,
        vocab_size: int,
        embedding_dim: int = 256,
        hidden_size: int = 512,
        num_layers: int = 2,
        dropout: float = 0.3,
        use_attention: bool = True
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Decoder
        
        Args:
            vocab_size: –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è
            embedding_dim: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            hidden_size: –†–∞–∑–º–µ—Ä —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è
            num_layers: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ—ë–≤ LSTM
            dropout: –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å dropout
            use_attention: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è
        """
        super(Decoder, self).__init__()
        
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.dropout = dropout
        self.use_attention = use_attention
        
        # –°–ª–æ–π —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        self.embedding = nn.Embedding(
            num_embeddings=vocab_size,
            embedding_dim=embedding_dim,
            padding_idx=0
        )
        
        # LSTM —Å–ª–æ–π
        # –ï—Å–ª–∏ –µ—Å—Ç—å attention, –≤—Ö–æ–¥–Ω–æ–π —Ä–∞–∑–º–µ—Ä —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è
        lstm_input_size = embedding_dim + hidden_size if use_attention else embedding_dim
        
        self.lstm = nn.LSTM(
            input_size=lstm_input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            dropout=dropout if num_layers > 1 else 0,
            batch_first=True
        )
        
        # –ú–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        if use_attention:
            self.attention = Attention(hidden_size)
        
        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π (–ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Å–ª–æ–≤–∞—Ä—é)
        fc_input_size = hidden_size * 2 if use_attention else hidden_size
        self.fc = nn.Linear(fc_input_size, vocab_size)
        
        # Dropout
        self.dropout_layer = nn.Dropout(dropout)


if __name__ == "__main__":
    """
    –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –±–∞–∑–æ–≤–æ–≥–æ Decoder
    """
    print("\n" + "=" * 60)
    print("–¢–ï–°–¢ DECODER - –ë–∞–∑–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞")
    print("=" * 60)
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
    vocab_size = 5000
    embedding_dim = 256
    hidden_size = 512
    num_layers = 2
    
    # –°–æ–∑–¥–∞—ë–º Decoder —Å attention
    decoder = Decoder(
        vocab_size=vocab_size,
        embedding_dim=embedding_dim,
        hidden_size=hidden_size,
        num_layers=num_layers,
        dropout=0.3,
        use_attention=True
    )
    
    print(f"‚úÖ Decoder —Å–æ–∑–¥–∞–Ω:")
    print(f"   Vocab size: {vocab_size}")
    print(f"   Embedding dim: {embedding_dim}")
    print(f"   Hidden size: {hidden_size}")
    print(f"   Num layers: {num_layers}")
    print(f"   Attention: –î–∞")
    print(f"\nüìä –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ –º–æ–¥–µ–ª–∏: {sum(p.numel() for p in decoder.parameters()):,}")
    
    print("\n" + "=" * 60)
    print("‚úÖ –ë–ê–ó–û–í–ê–Ø –°–¢–†–£–ö–¢–£–†–ê DECODER –ì–û–¢–û–í–ê")
    print("=" * 60)
