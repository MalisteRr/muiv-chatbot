#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–û–ë–£–ß–ï–ù–ò–ï RuBERT –î–õ–Ø –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò –ù–ê–ú–ï–†–ï–ù–ò–ô
–î–ª—è –¥–∏–ø–ª–æ–º–Ω–æ–π —Ä–∞–±–æ—Ç—ã - –ß–∞—Ç-–±–æ—Ç –¥–ª—è –∞–±–∏—Ç—É—Ä–∏–µ–Ω—Ç–æ–≤
"""

import json
import torch
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import os

class RuBERTTrainer:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è RuBERT –Ω–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –Ω–∞–º–µ—Ä–µ–Ω–∏–π
    """
    
    def __init__(self, data_file, model_name='DeepPavlov/rubert-base-cased'):
        print("="*70)
        print("üéì –û–ë–£–ß–ï–ù–ò–ï RuBERT –î–õ–Ø –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò –ù–ê–ú–ï–†–ï–ù–ò–ô")
        print("="*70)
        
        self.data_file = data_file
        self.model_name = model_name
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        
        print(f"\nüîß –ù–∞—Å—Ç—Ä–æ–π–∫–∏:")
        print(f"   - –ú–æ–¥–µ–ª—å: {model_name}")
        print(f"   - –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        print(f"   - –î–∞–Ω–Ω—ã–µ: {data_file}")
        
        # –°–æ–∑–¥–∞—ë–º –ø–∞–ø–∫—É –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        self.output_dir = f"rubert_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        os.makedirs(self.output_dir, exist_ok=True)
        os.makedirs(f"{self.output_dir}/plots", exist_ok=True)
        
        print(f"   - –í—ã—Ö–æ–¥–Ω–∞—è –ø–∞–ø–∫–∞: {self.output_dir}")
    
    def load_data(self):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ
        """
        print("\nüìÇ –ó–∞–≥—Ä—É–∂–∞—é –¥–∞–Ω–Ω—ã–µ...")
        
        with open(self.data_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(data)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        
        # –°–æ–∑–¥–∞—ë–º DataFrame
        df = pd.DataFrame(data)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö:")
        print(f"   - –í—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {len(df)}")
        print(f"   - –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {df['category'].nunique()}")
        print(f"   - –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö FAQ: {len(df[df['source'] == 'original'])}")
        print(f"   - –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö: {len(df[df['source'] != 'original'])}")
        
        print(f"\nüìã –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:")
        category_counts = df['category'].value_counts()
        for cat, count in category_counts.items():
            print(f"   - {cat}: {count} –ø—Ä–∏–º–µ—Ä–æ–≤ ({count/len(df)*100:.1f}%)")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥–∏—Å–±–∞–ª–∞–Ω—Å
        max_count = category_counts.max()
        min_count = category_counts.min()
        imbalance_ratio = max_count / min_count
        
        if imbalance_ratio > 5:
            print(f"\n‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –î–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ {imbalance_ratio:.1f}x")
            print("   –†–µ–∫–æ–º–µ–Ω–¥—É—é –¥–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ –ø—Ä–∏–º–µ—Ä–æ–≤ –≤ –º–∞–ª–µ–Ω—å–∫–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏")
        else:
            print(f"\n‚úÖ –ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ —Ö–æ—Ä–æ—à–∏–π ({imbalance_ratio:.1f}x)")
        
        return df
    
    def prepare_labels(self, df):
        """
        –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –º–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–æ–≤
        """
        print("\nüè∑Ô∏è –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞—é –º–µ—Ç–∫–∏...")
        
        # –°–æ–∑–¥–∞—ë–º –º–∞–ø–ø–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –≤ —á–∏—Å–ª–∞
        categories = sorted(df['category'].unique())
        self.label2id = {cat: i for i, cat in enumerate(categories)}
        self.id2label = {i: cat for cat, i in self.label2id.items()}
        
        # –î–æ–±–∞–≤–ª—è–µ–º —á–∏—Å–ª–æ–≤—ã–µ –º–µ—Ç–∫–∏
        df['label'] = df['category'].map(self.label2id)
        
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(categories)} –∫–ª–∞—Å—Å–æ–≤:")
        for cat, idx in self.label2id.items():
            print(f"   {idx}: {cat}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–∞–ø–ø–∏–Ω–≥
        with open(f"{self.output_dir}/label_mapping.json", 'w', encoding='utf-8') as f:
            json.dump({
                'label2id': self.label2id,
                'id2label': self.id2label
            }, f, ensure_ascii=False, indent=2)
        
        return df
    
    def split_data(self, df, test_size=0.2, val_size=0.1):
        """
        –†–∞–∑–±–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –Ω–∞ train/val/test
        """
        print(f"\n‚úÇÔ∏è –†–∞–∑–±–∏–≤–∞—é –¥–∞–Ω–Ω—ã–µ (train/val/test)...")
        
        # –°–Ω–∞—á–∞–ª–∞ –æ—Ç–¥–µ–ª—è–µ–º test
        train_val, test = train_test_split(
            df,
            test_size=test_size,
            stratify=df['label'],
            random_state=42
        )
        
        # –ü–æ—Ç–æ–º –æ—Ç–¥–µ–ª—è–µ–º validation –æ—Ç train
        val_size_adjusted = val_size / (1 - test_size)
        train, val = train_test_split(
            train_val,
            test_size=val_size_adjusted,
            stratify=train_val['label'],
            random_state=42
        )
        
        print(f"‚úÖ –†–∞–∑–±–∏–≤–∫–∞:")
        print(f"   - Train: {len(train)} –ø—Ä–∏–º–µ—Ä–æ–≤ ({len(train)/len(df)*100:.1f}%)")
        print(f"   - Val: {len(val)} –ø—Ä–∏–º–µ—Ä–æ–≤ ({len(val)/len(df)*100:.1f}%)")
        print(f"   - Test: {len(test)} –ø—Ä–∏–º–µ—Ä–æ–≤ ({len(test)/len(df)*100:.1f}%)")
        
        return train, val, test
    
    def tokenize_data(self, train, val, test):
        """
        –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ
        """
        print(f"\nüî§ –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É—é –¥–∞–Ω–Ω—ã–µ...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        
        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º
        def tokenize_function(examples):
            return self.tokenizer(
                examples['question'].tolist(),
                padding='max_length',
                truncation=True,
                max_length=128
            )
        
        print("   - Train...")
        train_encodings = tokenize_function(train)
        train_dataset = self.create_dataset(train_encodings, train['label'].tolist())
        
        print("   - Val...")
        val_encodings = tokenize_function(val)
        val_dataset = self.create_dataset(val_encodings, val['label'].tolist())
        
        print("   - Test...")
        test_encodings = tokenize_function(test)
        test_dataset = self.create_dataset(test_encodings, test['label'].tolist())
        
        print("‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        
        return train_dataset, val_dataset, test_dataset
    
    def create_dataset(self, encodings, labels):
        """
        –°–æ–∑–¥–∞—ë—Ç PyTorch Dataset
        """
        class IntentDataset(torch.utils.data.Dataset):
            def __init__(self, encodings, labels):
                self.encodings = encodings
                self.labels = labels
            
            def __getitem__(self, idx):
                item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
                item['labels'] = torch.tensor(self.labels[idx])
                return item
            
            def __len__(self):
                return len(self.labels)
        
        return IntentDataset(encodings, labels)
    
    def load_model(self):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å RuBERT
        """
        print(f"\nü§ñ –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å {self.model_name}...")
        
        self.model = AutoModelForSequenceClassification.from_pretrained(
            self.model_name,
            num_labels=len(self.label2id),
            id2label=self.id2label,
            label2id=self.label2id
        )
        
        self.model.to(self.device)
        
        # –°—á–∏—Ç–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
        print(f"   - –í—Å–µ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {total_params:,}")
        print(f"   - –û–±—É—á–∞–µ–º—ã—Ö: {trainable_params:,}")
    
    def compute_metrics(self, eval_pred):
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        """
        predictions, labels = eval_pred
        predictions = np.argmax(predictions, axis=1)
        
        accuracy = accuracy_score(labels, predictions)
        f1 = f1_score(labels, predictions, average='weighted')
        
        return {
            'accuracy': accuracy,
            'f1': f1
        }
    
    def train_model(self, train_dataset, val_dataset, epochs=5, batch_size=16, learning_rate=2e-5):
        """
        –û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å
        """
        print(f"\nüéØ –ù–∞—á–∏–Ω–∞—é –æ–±—É—á–µ–Ω–∏–µ...")
        print(f"   - –≠–ø–æ—Ö: {epochs}")
        print(f"   - Batch size: {batch_size}")
        print(f"   - Learning rate: {learning_rate}")
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è
        training_args = TrainingArguments(
            output_dir=f"{self.output_dir}/checkpoints",
            num_train_epochs=epochs,
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=batch_size,
            learning_rate=learning_rate,
            weight_decay=0.01,
            eval_strategy="epoch",  # –ò–∑–º–µ–Ω–µ–Ω–æ —Å evaluation_strategy
            save_strategy="epoch",
            load_best_model_at_end=True,
            metric_for_best_model="f1",
            logging_dir=f"{self.output_dir}/logs",
            logging_steps=10,
            save_total_limit=2,
            fp16=self.device == 'cuda',  # –ë—ã—Å—Ç—Ä–µ–µ –Ω–∞ GPU
        )
        
        # Trainer
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            compute_metrics=self.compute_metrics,
        )
        
        # –û–±—É—á–µ–Ω–∏–µ
        print("\n" + "="*70)
        print("üöÄ –¢–†–ï–ù–ò–†–û–í–ö–ê –ù–ê–ß–ê–õ–ê–°–¨!")
        print("="*70)
        
        train_result = trainer.train()
        
        print("\n" + "="*70)
        print("‚úÖ –¢–†–ï–ù–ò–†–û–í–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê!")
        print("="*70)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        trainer.save_model(f"{self.output_dir}/final_model")
        self.tokenizer.save_pretrained(f"{self.output_dir}/final_model")
        
        print(f"\nüíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {self.output_dir}/final_model")
        
        return trainer, train_result
    
    def evaluate_model(self, trainer, test_dataset, test_df):
        """
        –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        """
        print(f"\nüìä –û—Ü–µ–Ω–∏–≤–∞—é –º–æ–¥–µ–ª—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        predictions = trainer.predict(test_dataset)
        pred_labels = np.argmax(predictions.predictions, axis=1)
        true_labels = predictions.label_ids
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        accuracy = accuracy_score(true_labels, pred_labels)
        f1 = f1_score(true_labels, pred_labels, average='weighted')
        
        print(f"\nüéØ –§–ò–ù–ê–õ–¨–ù–´–ï –ú–ï–¢–†–ò–ö–ò:")
        print(f"   - Accuracy: {accuracy*100:.2f}%")
        print(f"   - F1-score: {f1*100:.2f}%")
        
        # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç
        print(f"\nüìã –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç –ø–æ –∫–ª–∞—Å—Å–∞–º:")
        report = classification_report(
            true_labels,
            pred_labels,
            target_names=[self.id2label[i] for i in range(len(self.id2label))],
            digits=3
        )
        print(report)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á—ë—Ç
        with open(f"{self.output_dir}/evaluation_report.txt", 'w', encoding='utf-8') as f:
            f.write(f"–§–ò–ù–ê–õ–¨–ù–´–ï –ú–ï–¢–†–ò–ö–ò\n")
            f.write(f"="*50 + "\n")
            f.write(f"Accuracy: {accuracy*100:.2f}%\n")
            f.write(f"F1-score: {f1*100:.2f}%\n\n")
            f.write(f"–î–ï–¢–ê–õ–¨–ù–´–ô –û–¢–ß–Å–¢\n")
            f.write(f"="*50 + "\n")
            f.write(report)
        
        # Confusion Matrix
        self.plot_confusion_matrix(true_labels, pred_labels)
        
        # –ü—Ä–∏–º–µ—Ä—ã –æ—à–∏–±–æ–∫
        self.show_errors(test_df, true_labels, pred_labels)
        
        return accuracy, f1
    
    def plot_confusion_matrix(self, true_labels, pred_labels):
        """
        –°—Ç—Ä–æ–∏—Ç confusion matrix
        """
        print(f"\nüìä –°—Ç—Ä–æ—é confusion matrix...")
        
        cm = confusion_matrix(true_labels, pred_labels)
        
        plt.figure(figsize=(12, 10))
        sns.heatmap(
            cm,
            annot=True,
            fmt='d',
            cmap='Blues',
            xticklabels=[self.id2label[i] for i in range(len(self.id2label))],
            yticklabels=[self.id2label[i] for i in range(len(self.id2label))]
        )
        plt.title('Confusion Matrix', fontsize=16)
        plt.ylabel('True Label', fontsize=12)
        plt.xlabel('Predicted Label', fontsize=12)
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        plt.tight_layout()
        plt.savefig(f"{self.output_dir}/plots/confusion_matrix.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {self.output_dir}/plots/confusion_matrix.png")
    
    def show_errors(self, test_df, true_labels, pred_labels, n=10):
        """
        –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø—Ä–∏–º–µ—Ä—ã –æ—à–∏–±–æ–∫
        """
        print(f"\n‚ùå –ü—Ä–∏–º–µ—Ä—ã –æ—à–∏–±–æ–∫ –º–æ–¥–µ–ª–∏ (–ø–µ—Ä–≤—ã–µ {n}):")
        
        test_df = test_df.reset_index(drop=True)
        errors = []
        
        for i, (true, pred) in enumerate(zip(true_labels, pred_labels)):
            if true != pred:
                errors.append({
                    'question': test_df.iloc[i]['question'],
                    'true_category': self.id2label[true],
                    'predicted_category': self.id2label[pred]
                })
        
        print(f"\n   –í—Å–µ–≥–æ –æ—à–∏–±–æ–∫: {len(errors)} –∏–∑ {len(true_labels)} ({len(errors)/len(true_labels)*100:.1f}%)")
        
        for i, error in enumerate(errors[:n], 1):
            print(f"\n   {i}. –í–æ–ø—Ä–æ—Å: {error['question'][:80]}...")
            print(f"      –ò—Å—Ç–∏–Ω–Ω–∞—è: {error['true_category']}")
            print(f"      –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∞: {error['predicted_category']}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –æ—à–∏–±–∫–∏
        with open(f"{self.output_dir}/errors.json", 'w', encoding='utf-8') as f:
            json.dump(errors, f, ensure_ascii=False, indent=2)
        
        print(f"\nüíæ –í—Å–µ –æ—à–∏–±–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {self.output_dir}/errors.json")
    
    def test_predictions(self):
        """
        –¢–µ—Å—Ç–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ –Ω–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö
        """
        print(f"\nüß™ –¢–ï–°–¢–ò–†–£–ï–ú –ú–û–î–ï–õ–¨ –ù–ê –ù–û–í–´–• –ü–†–ò–ú–ï–†–ê–•:")
        print("="*70)
        
        test_questions = [
            "–°–∫–æ–ª—å–∫–æ —Å—Ç–æ–∏—Ç —É—á—ë–±–∞?",
            "–ö–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω—É–∂–Ω—ã?",
            "–ï—Å—Ç—å –ª–∏ –æ–±—â–µ–∂–∏—Ç–∏–µ?",
            "–ú–æ–∂–Ω–æ –±–µ–∑ –ï–ì–≠ –ø–æ—Å—Ç—É–ø–∏—Ç—å?",
            "–ö–æ–≥–¥–∞ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –ø—Ä–∏—ë–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤?",
            "–ì–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç?",
            "–ï—Å—Ç—å –ª–∏ –±—é–¥–∂–µ—Ç–Ω—ã–µ –º–µ—Å—Ç–∞?",
            "–ú–æ–∂–Ω–æ –∑–∞–æ—á–Ω–æ —É—á–∏—Ç—å—Å—è?",
        ]
        
        for question in test_questions:
            inputs = self.tokenizer(
                question,
                return_tensors='pt',
                padding=True,
                truncation=True,
                max_length=128
            ).to(self.device)
            
            with torch.no_grad():
                outputs = self.model(**inputs)
                predictions = torch.softmax(outputs.logits, dim=1)
                predicted_class = torch.argmax(predictions, dim=1).item()
                confidence = predictions[0][predicted_class].item()
            
            print(f"\n‚ùì –í–æ–ø—Ä–æ—Å: {question}")
            print(f"‚úÖ –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {self.id2label[predicted_class]}")
            print(f"üìä –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence*100:.1f}%")
    
    def run(self, epochs=5, batch_size=16, learning_rate=2e-5):
        """
        –ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π pipeline –æ–±—É—á–µ–Ω–∏—è
        """
        start_time = datetime.now()
        
        # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        df = self.load_data()
        
        # 2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–µ—Ç–æ–∫
        df = self.prepare_labels(df)
        
        # 3. –†–∞–∑–±–∏–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        train, val, test = self.split_data(df)
        
        # 4. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        train_dataset, val_dataset, test_dataset = self.tokenize_data(train, val, test)
        
        # 5. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        self.load_model()
        
        # 6. –û–±—É—á–µ–Ω–∏–µ
        trainer, train_result = self.train_model(
            train_dataset,
            val_dataset,
            epochs=epochs,
            batch_size=batch_size,
            learning_rate=learning_rate
        )
        
        # 7. –û—Ü–µ–Ω–∫–∞
        accuracy, f1 = self.evaluate_model(trainer, test_dataset, test)
        
        # 8. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
        self.test_predictions()
        
        # –ò—Ç–æ–≥–∏
        elapsed = datetime.now() - start_time
        
        print("\n" + "="*70)
        print("üéâ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
        print("="*70)
        print(f"‚è±Ô∏è –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {elapsed}")
        print(f"üéØ Accuracy: {accuracy*100:.2f}%")
        print(f"üìä F1-score: {f1*100:.2f}%")
        print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {self.output_dir}/final_model")
        print(f"üìÅ –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã: {self.output_dir}/")
        print("="*70)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º summary
        summary = {
            'model': self.model_name,
            'data_file': self.data_file,
            'total_examples': len(df),
            'num_classes': len(self.label2id),
            'train_examples': len(train),
            'val_examples': len(val),
            'test_examples': len(test),
            'epochs': epochs,
            'batch_size': batch_size,
            'learning_rate': learning_rate,
            'accuracy': float(accuracy),
            'f1_score': float(f1),
            'training_time': str(elapsed),
            'device': self.device
        }
        
        with open(f"{self.output_dir}/training_summary.json", 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        print(f"\nüìä Summary —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {self.output_dir}/training_summary.json")


def main():
    print("\n" + "="*70)
    print("   –û–ë–£–ß–ï–ù–ò–ï RuBERT –î–õ–Ø –î–ò–ü–õ–û–ú–ù–û–ô –†–ê–ë–û–¢–´")
    print("="*70)
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
    data_file = input("\nüìÇ –§–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä: final_dataset.json): ").strip()
    if not data_file:
        data_file = "final_dataset.json"
    
    print(f"\n‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è:")
    print("   (–Ω–∞–∂–º–∏ Enter –¥–ª—è –∑–Ω–∞—á–µ–Ω–∏–π –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)")
    
    epochs = input("   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 5): ").strip()
    epochs = int(epochs) if epochs else 5
    
    batch_size = input("   Batch size (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 16): ").strip()
    batch_size = int(batch_size) if batch_size else 16
    
    learning_rate = input("   Learning rate (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 2e-5): ").strip()
    learning_rate = float(learning_rate) if learning_rate else 2e-5
    
    print(f"\n‚úÖ –ù–∞—Å—Ç—Ä–æ–π–∫–∏:")
    print(f"   - –î–∞–Ω–Ω—ã–µ: {data_file}")
    print(f"   - –≠–ø–æ—Ö–∏: {epochs}")
    print(f"   - Batch size: {batch_size}")
    print(f"   - Learning rate: {learning_rate}")
    
    input("\nüëâ –ù–∞–∂–º–∏ Enter —á—Ç–æ–±—ã –Ω–∞—á–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ...")
    
    # –ó–∞–ø—É—Å–∫
    trainer = RuBERTTrainer(data_file)
    trainer.run(epochs=epochs, batch_size=batch_size, learning_rate=learning_rate)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º!")
    except Exception as e:
        print(f"\n‚ùå –û–®–ò–ë–ö–ê: {e}")
        import traceback
        traceback.print_exc()
