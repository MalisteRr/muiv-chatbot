"""
Seq2Seq –º–æ–¥–µ–ª—å (Encoder-Decoder)

Seq2Seq –º–æ–¥–µ–ª—å –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç Encoder –∏ Decoder –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã.

–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
–í–æ–ø—Ä–æ—Å ‚Üí Encoder ‚Üí –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –≤–µ–∫—Ç–æ—Ä ‚Üí Decoder ‚Üí –û—Ç–≤–µ—Ç
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import random

from .encoder import Encoder
from .decoder import Decoder


class Seq2Seq(nn.Module):
    """
    Seq2Seq –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤
    """
    
    def __init__(
        self,
        encoder: Encoder,
        decoder: Decoder,
        device: str = 'cpu'
    ):
        super(Seq2Seq, self).__init__()
        
        self.encoder = encoder
        self.decoder = decoder
        self.device = device
        
        assert encoder.hidden_size == decoder.hidden_size, \
            "Hidden size encoder'–∞ –∏ decoder'–∞ –¥–æ–ª–∂–Ω—ã —Å–æ–≤–ø–∞–¥–∞—Ç—å!"
        assert encoder.num_layers == decoder.num_layers, \
            "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ—ë–≤ encoder'–∞ –∏ decoder'–∞ –¥–æ–ª–∂–Ω–æ —Å–æ–≤–ø–∞–¥–∞—Ç—å!"
    
    def forward(self, src, trg, src_lengths=None, teacher_forcing_ratio: float = 0.5):
        """–ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ Seq2Seq (–æ–±—É—á–µ–Ω–∏–µ)"""
        batch_size = src.size(0)
        trg_len = trg.size(1)
        trg_vocab_size = self.decoder.vocab_size
        
        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)
        encoder_outputs, hidden, cell = self.encoder(src, src_lengths)
        decoder_input = trg[:, 0].unsqueeze(1)
        
        for t in range(1, trg_len):
            prediction, hidden, cell, _ = self.decoder(
                decoder_input, hidden, cell,
                encoder_outputs if self.decoder.use_attention else None
            )
            
            outputs[:, t, :] = prediction
            use_teacher_forcing = random.random() < teacher_forcing_ratio
            top_prediction = prediction.argmax(1)
            
            decoder_input = trg[:, t].unsqueeze(1) if use_teacher_forcing else top_prediction.unsqueeze(1)
        
        return outputs
    
    def generate(self, src, src_lengths=None, max_length: int = 100, sos_token: int = 2, eos_token: int = 3):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ (inference)"""
        self.eval()
        batch_size = src.size(0)
        
        with torch.no_grad():
            encoder_outputs, hidden, cell = self.encoder(src, src_lengths)
            decoder_input = torch.full((batch_size, 1), sos_token, dtype=torch.long).to(self.device)
            generated_tokens = []
            finished = torch.zeros(batch_size, dtype=torch.bool).to(self.device)
            
            for _ in range(max_length):
                prediction, hidden, cell, _ = self.decoder(
                    decoder_input, hidden, cell,
                    encoder_outputs if self.decoder.use_attention else None
                )
                
                next_token = prediction.argmax(1)
                generated_tokens.append(next_token.unsqueeze(1))
                finished = finished | (next_token == eos_token)
                
                if finished.all():
                    break
                
                decoder_input = next_token.unsqueeze(1)
            
            generated_tokens = torch.cat(generated_tokens, dim=1)
        
        return generated_tokens
    
    def count_parameters(self):
        """–ü–æ–¥—Å—á—ë—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)


def init_weights(model):
    """
    –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Xavier uniform –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é
    
    Args:
        model: –ú–æ–¥–µ–ª—å –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
    """
    for name, param in model.named_parameters():
        if 'weight' in name:
            nn.init.xavier_uniform_(param.data)
        elif 'bias' in name:
            nn.init.constant_(param.data, 0)


def create_seq2seq_model(
    vocab_size: int,
    embedding_dim: int = 256,
    hidden_size: int = 512,
    num_layers: int = 2,
    dropout: float = 0.3,
    use_attention: bool = True,
    device: str = 'cpu'
):
    """
    –§–∞–±—Ä–∏—á–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è Seq2Seq –º–æ–¥–µ–ª–∏
    
    Args:
        vocab_size: –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è
        embedding_dim: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        hidden_size: –†–∞–∑–º–µ—Ä —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è
        num_layers: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ—ë–≤ LSTM
        dropout: –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å dropout
        use_attention: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è
        device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (cpu –∏–ª–∏ cuda)
    
    Returns:
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å Seq2Seq
    """
    # –°–æ–∑–¥–∞—ë–º encoder
    encoder = Encoder(
        vocab_size=vocab_size,
        embedding_dim=embedding_dim,
        hidden_size=hidden_size,
        num_layers=num_layers,
        dropout=dropout
    )
    
    # –°–æ–∑–¥–∞—ë–º decoder
    decoder = Decoder(
        vocab_size=vocab_size,
        embedding_dim=embedding_dim,
        hidden_size=hidden_size,
        num_layers=num_layers,
        dropout=dropout,
        use_attention=use_attention
    )
    
    # –°–æ–∑–¥–∞—ë–º seq2seq –º–æ–¥–µ–ª—å
    model = Seq2Seq(encoder, decoder, device)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–µ—Å–∞
    model.apply(init_weights)
    
    # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
    model = model.to(device)
    
    print(f"‚úÖ Seq2Seq –º–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞:")
    print(f"   –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {model.count_parameters():,}")
    print(f"   –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    print(f"   Attention: {'–î–∞' if use_attention else '–ù–µ—Ç'}")
    
    return model


if __name__ == "__main__":
    """
    –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–ª–Ω–æ–π Seq2Seq –º–æ–¥–µ–ª–∏
    """
    print("\n" + "=" * 60)
    print("–¢–ï–°–¢ SEQ2SEQ - –ü–æ–ª–Ω–∞—è –º–æ–¥–µ–ª—å")
    print("=" * 60)
    
    # –°–æ–∑–¥–∞—ë–º –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ —Ñ–∞–±—Ä–∏—á–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é
    model = create_seq2seq_model(
        vocab_size=5000,
        embedding_dim=256,
        hidden_size=512,
        num_layers=2,
        dropout=0.3,
        use_attention=True,
        device='cpu'
    )
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    batch_size = 4
    src = torch.randint(0, 5000, (batch_size, 20))
    trg = torch.randint(0, 5000, (batch_size, 25))
    src_lengths = torch.tensor([20, 18, 15, 12])
    
    print(f"\nüß™ –¢–µ—Å—Ç –æ–±—É—á–µ–Ω–∏—è:")
    model.eval()
    with torch.no_grad():
        outputs = model(src, trg, src_lengths, teacher_forcing_ratio=1.0)
    print(f"   –í—ã—Ö–æ–¥: {outputs.shape}")
    
    print(f"\nü§ñ –¢–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:")
    with torch.no_grad():
        generated = model.generate(src, src_lengths, max_length=30)
    print(f"   –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ: {generated.shape}")
    
    print("\n" + "=" * 60)
    print("‚úÖ SEQ2SEQ –ü–û–õ–ù–û–°–¢–¨–Æ –ì–û–¢–û–í")
    print("=" * 60)
