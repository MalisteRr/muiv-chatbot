"""
Seq2Seq –º–æ–¥–µ–ª—å (Encoder-Decoder)

Seq2Seq –º–æ–¥–µ–ª—å –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç Encoder –∏ Decoder –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã.

–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
–í–æ–ø—Ä–æ—Å ‚Üí Encoder ‚Üí –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –≤–µ–∫—Ç–æ—Ä ‚Üí Decoder ‚Üí –û—Ç–≤–µ—Ç
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import random

from .encoder import Encoder
from .decoder import Decoder


class Seq2Seq(nn.Module):
    """
    Seq2Seq –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤
    """
    
    def __init__(
        self,
        encoder: Encoder,
        decoder: Decoder,
        device: str = 'cpu'
    ):
        super(Seq2Seq, self).__init__()
        
        self.encoder = encoder
        self.decoder = decoder
        self.device = device
        
        assert encoder.hidden_size == decoder.hidden_size, \
            "Hidden size encoder'–∞ –∏ decoder'–∞ –¥–æ–ª–∂–Ω—ã —Å–æ–≤–ø–∞–¥–∞—Ç—å!"
        assert encoder.num_layers == decoder.num_layers, \
            "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ—ë–≤ encoder'–∞ –∏ decoder'–∞ –¥–æ–ª–∂–Ω–æ —Å–æ–≤–ø–∞–¥–∞—Ç—å!"
    
    def forward(
        self, 
        src, 
        trg, 
        src_lengths=None,
        teacher_forcing_ratio: float = 0.5
    ):
        """
        –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ Seq2Seq (–æ–±—É—á–µ–Ω–∏–µ)
        """
        batch_size = src.size(0)
        trg_len = trg.size(1)
        trg_vocab_size = self.decoder.vocab_size
        
        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)
        
        # Encoder
        encoder_outputs, hidden, cell = self.encoder(src, src_lengths)
        
        # Decoder
        decoder_input = trg[:, 0].unsqueeze(1)
        
        for t in range(1, trg_len):
            prediction, hidden, cell, attention_weights = self.decoder(
                decoder_input,
                hidden,
                cell,
                encoder_outputs if self.decoder.use_attention else None
            )
            
            outputs[:, t, :] = prediction
            
            use_teacher_forcing = random.random() < teacher_forcing_ratio
            top_prediction = prediction.argmax(1)
            
            if use_teacher_forcing:
                decoder_input = trg[:, t].unsqueeze(1)
            else:
                decoder_input = top_prediction.unsqueeze(1)
        
        return outputs
    
    def generate(
        self, 
        src, 
        src_lengths=None,
        max_length: int = 100,
        sos_token: int = 2,
        eos_token: int = 3
    ):
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ (inference/—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ)
        
        Args:
            src: –í—Ö–æ–¥–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å (–≤–æ–ø—Ä–æ—Å)
                –§–æ—Ä–º–∞: (batch_size, src_seq_length)
            src_lengths: –†–µ–∞–ª—å–Ω—ã–µ –¥–ª–∏–Ω—ã –≤—Ö–æ–¥–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
            max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
            sos_token: –ò–Ω–¥–µ–∫—Å —Ç–æ–∫–µ–Ω–∞ <SOS>
            eos_token: –ò–Ω–¥–µ–∫—Å —Ç–æ–∫–µ–Ω–∞ <EOS>
        
        Returns:
            generated_tokens: –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
                            –§–æ—Ä–º–∞: (batch_size, generated_length)
        """
        self.eval()
        
        batch_size = src.size(0)
        
        with torch.no_grad():
            # 1. Encoder
            encoder_outputs, hidden, cell = self.encoder(src, src_lengths)
            
            # 2. Decoder - –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
            
            # –ù–∞—á–∏–Ω–∞–µ–º —Å <SOS> —Ç–æ–∫–µ–Ω–∞
            decoder_input = torch.full(
                (batch_size, 1), 
                sos_token, 
                dtype=torch.long
            ).to(self.device)
            
            # –°–ø–∏—Å–æ–∫ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
            generated_tokens = []
            
            # –§–ª–∞–≥ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ –≤ –±–∞—Ç—á–µ
            finished = torch.zeros(batch_size, dtype=torch.bool).to(self.device)
            
            for _ in range(max_length):
                # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º —Å–ª–µ–¥—É—é—â–∏–π —Ç–æ–∫–µ–Ω
                prediction, hidden, cell, _ = self.decoder(
                    decoder_input,
                    hidden,
                    cell,
                    encoder_outputs if self.decoder.use_attention else None
                )
                
                # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–∫–µ–Ω —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é
                next_token = prediction.argmax(1)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω
                generated_tokens.append(next_token.unsqueeze(1))
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å—Ç—Ä–µ—Ç–∏–ª—Å—è –ª–∏ <EOS> —Ç–æ–∫–µ–Ω
                finished = finished | (next_token == eos_token)
                
                # –ï—Å–ª–∏ –≤—Å–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∑–∞–≤–µ—Ä—à–∏–ª–∏—Å—å - –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è
                if finished.all():
                    break
                
                # –°–ª–µ–¥—É—é—â–∏–π –≤—Ö–æ–¥ –¥–ª—è decoder'–∞
                decoder_input = next_token.unsqueeze(1)
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Ç–æ–∫–µ–Ω—ã
            generated_tokens = torch.cat(generated_tokens, dim=1)
        
        return generated_tokens
    
    def count_parameters(self):
        """–ü–æ–¥—Å—á—ë—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)


if __name__ == "__main__":
    """
    –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Seq2Seq —Å generate
    """
    print("\n" + "=" * 60)
    print("–¢–ï–°–¢ SEQ2SEQ - –ú–µ—Ç–æ–¥ generate")
    print("=" * 60)
    
    vocab_size = 5000
    batch_size = 4
    src_len = 20
    
    encoder = Encoder(vocab_size=vocab_size)
    decoder = Decoder(vocab_size=vocab_size, use_attention=True)
    model = Seq2Seq(encoder, decoder, device='cpu')
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    src = torch.randint(0, vocab_size, (batch_size, src_len))
    src_lengths = torch.tensor([20, 18, 15, 12])
    
    print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞")
    print(f"\nüß™ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞:")
    print(f"   –í—Ö–æ–¥: {src.shape}")
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
    with torch.no_grad():
        generated = model.generate(src, src_lengths, max_length=30)
    
    print(f"\nüì§ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ:")
    print(f"   –§–æ—Ä–º–∞: {generated.shape}")
    print(f"   –ü–µ—Ä–≤–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å (–ø–µ—Ä–≤—ã–µ 10): {generated[0][:10].tolist()}")
    
    print("\n" + "=" * 60)
    print("‚úÖ GENERATE –†–ê–ë–û–¢–ê–ï–¢")
    print("=" * 60)
