"""
Seq2Seq –º–æ–¥–µ–ª—å (Encoder-Decoder)

Seq2Seq –º–æ–¥–µ–ª—å –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç Encoder –∏ Decoder –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã.

–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
–í–æ–ø—Ä–æ—Å ‚Üí Encoder ‚Üí –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –≤–µ–∫—Ç–æ—Ä ‚Üí Decoder ‚Üí –û—Ç–≤–µ—Ç
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import random

from .encoder import Encoder
from .decoder import Decoder


class Seq2Seq(nn.Module):
    """
    Seq2Seq –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤
    """
    
    def __init__(
        self,
        encoder: Encoder,
        decoder: Decoder,
        device: str = 'cpu'
    ):
        super(Seq2Seq, self).__init__()
        
        self.encoder = encoder
        self.decoder = decoder
        self.device = device
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        assert encoder.hidden_size == decoder.hidden_size, \
            "Hidden size encoder'–∞ –∏ decoder'–∞ –¥–æ–ª–∂–Ω—ã —Å–æ–≤–ø–∞–¥–∞—Ç—å!"
        assert encoder.num_layers == decoder.num_layers, \
            "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ—ë–≤ encoder'–∞ –∏ decoder'–∞ –¥–æ–ª–∂–Ω–æ —Å–æ–≤–ø–∞–¥–∞—Ç—å!"
    
    def forward(
        self, 
        src, 
        trg, 
        src_lengths=None,
        teacher_forcing_ratio: float = 0.5
    ):
        """
        –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ Seq2Seq (–æ–±—É—á–µ–Ω–∏–µ)
        
        Args:
            src: –í—Ö–æ–¥–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å (–≤–æ–ø—Ä–æ—Å)
                –§–æ—Ä–º–∞: (batch_size, src_seq_length)
            trg: –¶–µ–ª–µ–≤–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å (–æ—Ç–≤–µ—Ç)
                –§–æ—Ä–º–∞: (batch_size, trg_seq_length)
            src_lengths: –†–µ–∞–ª—å–Ω—ã–µ –¥–ª–∏–Ω—ã –≤—Ö–æ–¥–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
            teacher_forcing_ratio: –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è teacher forcing
                                  1.0 = –≤—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω
                                  0.0 = –≤—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        
        Returns:
            outputs: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —à–∞–≥–∞
                    –§–æ—Ä–º–∞: (batch_size, trg_seq_length, vocab_size)
        """
        batch_size = src.size(0)
        trg_len = trg.size(1)
        trg_vocab_size = self.decoder.vocab_size
        
        # –¢–µ–Ω–∑–æ—Ä –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—ã—Ö–æ–¥–æ–≤ decoder'–∞
        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)
        
        # 1. ENCODER: –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Ö–æ–¥–Ω—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        encoder_outputs, hidden, cell = self.encoder(src, src_lengths)
        
        # 2. DECODER: –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —Ç–æ–∫–µ–Ω –∑–∞ —Ç–æ–∫–µ–Ω–æ–º
        
        # –ü–µ—Ä–≤—ã–π —Ç–æ–∫–µ–Ω decoder'–∞ - —ç—Ç–æ –≤—Å–µ–≥–¥–∞ <SOS> (Start Of Sequence)
        decoder_input = trg[:, 0].unsqueeze(1)  # (batch_size, 1)
        
        for t in range(1, trg_len):
            # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º —Å–ª–µ–¥—É—é—â–∏–π —Ç–æ–∫–µ–Ω
            prediction, hidden, cell, attention_weights = self.decoder(
                decoder_input,
                hidden,
                cell,
                encoder_outputs if self.decoder.use_attention else None
            )
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            outputs[:, t, :] = prediction
            
            # –†–µ—à–∞–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ teacher forcing
            use_teacher_forcing = random.random() < teacher_forcing_ratio
            
            # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–∫–µ–Ω —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é
            top_prediction = prediction.argmax(1)
            
            # –í—ã–±–∏—Ä–∞–µ–º –≤—Ö–æ–¥–Ω–æ–π —Ç–æ–∫–µ–Ω –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —à–∞–≥–∞
            if use_teacher_forcing:
                # Teacher forcing: –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω
                decoder_input = trg[:, t].unsqueeze(1)
            else:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
                decoder_input = top_prediction.unsqueeze(1)
        
        return outputs
    
    def count_parameters(self):
        """–ü–æ–¥—Å—á—ë—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)


if __name__ == "__main__":
    """
    –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Seq2Seq —Å forward
    """
    print("\n" + "=" * 60)
    print("–¢–ï–°–¢ SEQ2SEQ - –ú–µ—Ç–æ–¥ forward")
    print("=" * 60)
    
    vocab_size = 5000
    batch_size = 4
    src_len = 20
    trg_len = 25
    
    # –°–æ–∑–¥–∞—ë–º –º–æ–¥–µ–ª—å
    encoder = Encoder(vocab_size=vocab_size)
    decoder = Decoder(vocab_size=vocab_size, use_attention=True)
    model = Seq2Seq(encoder, decoder, device='cpu')
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    src = torch.randint(0, vocab_size, (batch_size, src_len))
    trg = torch.randint(0, vocab_size, (batch_size, trg_len))
    src_lengths = torch.tensor([20, 18, 15, 12])
    
    print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞: {model.count_parameters():,} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
    print(f"\nüß™ –¢–µ—Å—Ç–æ–≤—ã–π –≤—Ö–æ–¥:")
    print(f"   Source: {src.shape}")
    print(f"   Target: {trg.shape}")
    
    # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
    model.eval()
    with torch.no_grad():
        outputs = model(src, trg, src_lengths, teacher_forcing_ratio=1.0)
    
    print(f"\nüì§ –í—ã—Ö–æ–¥:")
    print(f"   –§–æ—Ä–º–∞: {outputs.shape}")
    print(f"   –û–∂–∏–¥–∞–ª–æ—Å—å: ({batch_size}, {trg_len}, {vocab_size})")
    
    print("\n" + "=" * 60)
    print("‚úÖ FORWARD –†–ê–ë–û–¢–ê–ï–¢")
    print("=" * 60)
