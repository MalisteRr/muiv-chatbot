"""
–°–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π LSTM –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å Self-Attention
–†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–æ –¥–ª—è –í–ö–†: –°–∏–Ω–∏—Ü–∏–Ω –ú.–î.

–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π —Å–∏—Å—Ç–µ–º–æ–π muiv-chatbot
–ú–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –∫–∞–∫ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ RuBERT
"""

import logging
import json
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass, asdict

import torch
import torch.nn as nn

logger = logging.getLogger(__name__)


# ================== –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ==================

CATEGORY_MAPPING = {
    "–û–±—É—á–µ–Ω–∏–µ": 0,
    "–ü–æ—Å—Ç—É–ø–ª–µ–Ω–∏–µ": 1,
    "–û–±—â–µ–∂–∏—Ç–∏–µ": 2,
    "–§–æ—Ä–º—ã –æ–±—É—á–µ–Ω–∏—è": 3,
    "–°—Ç–æ–∏–º–æ—Å—Ç—å": 4,
    "–ë–µ–∑ –ï–ì–≠": 5,
    "–ö–æ–Ω—Ç–∞–∫—Ç—ã": 6,
    "–ë—é–¥–∂–µ—Ç": 7
}
CATEGORY_NAMES = list(CATEGORY_MAPPING.keys())
ID2LABEL = {v: k for k, v in CATEGORY_MAPPING.items()}


# ================== MODEL CONFIG ==================

@dataclass
class ModelConfig:
    vocab_size: int = 9846
    embedding_dim: int = 100
    hidden_size: int = 256
    num_layers: int = 2
    num_classes: int = 8
    dropout: float = 0.4
    bidirectional: bool = True
    max_seq_length: int = 64
    attention_heads: int = 4
    
    def save(self, path: str):
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(asdict(self), f, indent=2)
    
    @classmethod
    def load(cls, path: str) -> 'ModelConfig':
        with open(path, 'r', encoding='utf-8') as f:
            return cls(**json.load(f))


# ================== TOKENIZER ==================

class TextTokenizer:
    """–¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è LSTM –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞"""
    
    def __init__(self, max_seq_length: int = 64):
        self.max_seq_length = max_seq_length
        self.word2idx = {'<PAD>': 0, '<UNK>': 1}
        self.idx2word = {0: '<PAD>', 1: '<UNK>'}
    
    def encode(self, text: str) -> Tuple[torch.Tensor, torch.Tensor]:
        words = text.lower().split()[:self.max_seq_length]
        ids = [self.word2idx.get(w, 1) for w in words]
        
        pad_len = self.max_seq_length - len(ids)
        attention_mask = [1.0] * len(ids) + [0.0] * pad_len
        ids = ids + [0] * pad_len
        
        return torch.tensor(ids), torch.tensor(attention_mask)
    
    def save(self, path: str):
        with open(path, 'w', encoding='utf-8') as f:
            json.dump({
                'word2idx': self.word2idx,
                'max_seq_length': self.max_seq_length
            }, f, ensure_ascii=False, indent=2)
    
    @classmethod
    def load(cls, path: str) -> 'TextTokenizer':
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        tok = cls(max_seq_length=data['max_seq_length'])
        tok.word2idx = data['word2idx']
        tok.idx2word = {int(v): k for k, v in data['word2idx'].items()}
        return tok


# ================== MODEL ==================

class SelfAttention(nn.Module):
    """Multi-Head Self-Attention –º–µ—Ö–∞–Ω–∏–∑–º"""
    
    def __init__(self, hidden_size: int, num_heads: int = 4, dropout: float = 0.1):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads
        self.scale = self.head_dim ** -0.5
        
        self.query = nn.Linear(hidden_size, hidden_size)
        self.key = nn.Linear(hidden_size, hidden_size)
        self.value = nn.Linear(hidden_size, hidden_size)
        self.out = nn.Linear(hidden_size, hidden_size)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x, mask=None):
        B, L, H = x.shape
        
        q = self.query(x).view(B, L, self.num_heads, self.head_dim).transpose(1, 2)
        k = self.key(x).view(B, L, self.num_heads, self.head_dim).transpose(1, 2)
        v = self.value(x).view(B, L, self.num_heads, self.head_dim).transpose(1, 2)
        
        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale
        
        if mask is not None:
            mask = mask.unsqueeze(1).unsqueeze(2)
            scores = scores.masked_fill(mask == 0, -1e9)
        
        attn = torch.softmax(scores, dim=-1)
        attn = self.dropout(attn)
        
        out = torch.matmul(attn, v)
        out = out.transpose(1, 2).contiguous().view(B, L, H)
        
        return self.out(out)


class LSTMClassifier(nn.Module):
    """LSTM –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å Self-Attention"""
    
    def __init__(self, config: ModelConfig, pretrained_embeddings=None):
        super().__init__()
        self.config = config
        
        self.embedding = nn.Embedding(config.vocab_size, config.embedding_dim, padding_idx=0)
        if pretrained_embeddings is not None:
            self.embedding.weight.data.copy_(pretrained_embeddings)
        
        self.lstm = nn.LSTM(
            config.embedding_dim,
            config.hidden_size,
            num_layers=config.num_layers,
            batch_first=True,
            dropout=config.dropout if config.num_layers > 1 else 0,
            bidirectional=config.bidirectional
        )
        
        lstm_output_size = config.hidden_size * (2 if config.bidirectional else 1)
        
        self.attention = SelfAttention(lstm_output_size, config.attention_heads, config.dropout)
        self.layer_norm = nn.LayerNorm(lstm_output_size)
        
        self.classifier = nn.Sequential(
            nn.Linear(lstm_output_size, 256),
            nn.ReLU(),
            nn.Dropout(config.dropout),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(config.dropout),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(config.dropout / 2),
            nn.Linear(64, config.num_classes)
)

    
    def forward(self, input_ids, attention_mask=None):
        x = self.embedding(input_ids)
        lstm_out, _ = self.lstm(x)
        
        attn_out = self.attention(lstm_out, attention_mask)
        x = self.layer_norm(lstm_out + attn_out)
        
        if attention_mask is not None:
            mask = attention_mask.unsqueeze(-1)
            x = (x * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1)
        else:
            x = x.mean(dim=1)
        
        return self.classifier(x)


# ================== CLASSIFIER WRAPPER ==================

class CustomIntentClassifier:
    """
    –û–±—ë—Ä—Ç–∫–∞ –Ω–∞–¥ LSTM –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–º –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å RuBERT –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º
    """
    
    def __init__(self, model_path: str, confidence_threshold: float = 0.7):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
        
        Args:
            model_path: –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –º–æ–¥–µ–ª—å—é (classifier.pt, tokenizer.json, config.json)
            confidence_threshold: –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ (0-1)
        """
        self.model_path = Path(model_path)
        self.confidence_threshold = confidence_threshold
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        
        self.id2label = ID2LABEL
        self.label2id = CATEGORY_MAPPING
        
        self._load_model()
    
    def _load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞"""
        try:
            logger.info(f"üß† –ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–π LSTM –º–æ–¥–µ–ª–∏ –∏–∑ {self.model_path}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            tokenizer_path = self.model_path / "tokenizer.json"
            self.tokenizer = TextTokenizer.load(str(tokenizer_path))
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥
            config_path = self.model_path / "config.json"
            self.config = ModelConfig.load(str(config_path))
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            model_file = self.model_path / "classifier.pt"
            checkpoint = torch.load(model_file, map_location=self.device)
            
            self.model = LSTMClassifier(self.config)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.model.to(self.device)
            self.model.eval()
            
            logger.info(f"‚úÖ –°–æ–±—Å—Ç–≤–µ–Ω–Ω–∞—è LSTM –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
            logger.info(f"   üìã –ö–∞—Ç–µ–≥–æ—Ä–∏–π: {len(self.id2label)}")
            logger.info(f"   üíª –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
            logger.info(f"   üéØ –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏: {self.confidence_threshold}")
            logger.info(f"   üìä –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {sum(p.numel() for p in self.model.parameters()):,}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ LSTM –º–æ–¥–µ–ª–∏: {e}", exc_info=True)
            raise
    
    def predict(self, text: str) -> Dict:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é –¥–ª—è —Ç–µ–∫—Å—Ç–∞
        
        Args:
            text: –¢–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            
        Returns:
            dict: {
                'category': str,           # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è
                'confidence': float,       # –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (0-1)
                'is_confident': bool,      # –í—ã—à–µ –ª–∏ –ø–æ—Ä–æ–≥–∞
                'all_scores': dict         # –í—Å–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º–∏
            }
        """
        try:
            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
            input_ids, attention_mask = self.tokenizer.encode(text)
            input_ids = input_ids.unsqueeze(0).to(self.device)
            attention_mask = attention_mask.unsqueeze(0).to(self.device)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            with torch.no_grad():
                logits = self.model(input_ids, attention_mask)
                probabilities = torch.softmax(logits, dim=1)[0]
            
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å
            predicted_id = torch.argmax(probabilities).item()
            confidence = probabilities[predicted_id].item()
            predicted_category = self.id2label[predicted_id]
            
            # –í—Å–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
            all_scores = {
                self.id2label[i]: probabilities[i].item()
                for i in range(len(probabilities))
            }
            
            result = {
                'category': predicted_category,
                'confidence': confidence,
                'is_confident': confidence >= self.confidence_threshold,
                'all_scores': all_scores
            }
            
            logger.debug(
                f"LSTM –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: '{text[:50]}...' ‚Üí {predicted_category} "
                f"({confidence*100:.1f}%)"
            )
            
            return result
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è LSTM: {e}", exc_info=True)
            return {
                'category': None,
                'confidence': 0.0,
                'is_confident': False,
                'all_scores': {}
            }
    
    def get_top_categories(self, text: str, top_k: int = 3) -> list:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Ç–æ–ø-K –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        """
        prediction = self.predict(text)
        sorted_scores = sorted(
            prediction['all_scores'].items(),
            key=lambda x: x[1],
            reverse=True
        )
        return sorted_scores[:top_k]


# ================== –ì–õ–û–ë–ê–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò ==================

_custom_classifier_instance: Optional[CustomIntentClassifier] = None


def init_custom_classifier(model_path: str, confidence_threshold: float = 0.7):
    """
    –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
    
    Args:
        model_path: –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏
        confidence_threshold: –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
    """
    global _custom_classifier_instance
    
    try:
        _custom_classifier_instance = CustomIntentClassifier(model_path, confidence_threshold)
        logger.info("‚úÖ –°–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π LSTM –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    except Exception as e:
        logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å LSTM –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä: {e}")
        _custom_classifier_instance = None


def get_custom_classifier() -> Optional[CustomIntentClassifier]:
    """
    –ü–æ–ª—É—á–∏—Ç—å –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
    """
    return _custom_classifier_instance


def is_custom_classifier_available() -> bool:
    """
    –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–µ–Ω –ª–∏ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
    """
    return _custom_classifier_instance is not None