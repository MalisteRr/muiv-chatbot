"""
Inference –º–æ–¥—É–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–Ω–æ–π ML –º–æ–¥–µ–ª–∏

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—É—á–µ–Ω–Ω—É—é Seq2Seq –º–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤.
–ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ —É–≤–µ—Ä–µ–Ω–∞ - fallback –Ω–∞ DeepSeek API.
"""

import torch
import os
from typing import Optional, Tuple

from ml.models import (
    Seq2Seq,
    Encoder,
    Decoder,
    SimpleTokenizer,
    ModelConfig
)


class MLModelInference:
    """–ö–ª–∞—Å—Å –¥–ª—è inference –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    
    def __init__(
        self,
        model_path: str = None,
        tokenizer_path: str = None,
        device: str = None,
        confidence_threshold: float = 0.6
    ):
        self.model_path = model_path or ModelConfig.MODEL_SAVE_PATH
        self.tokenizer_path = tokenizer_path or ModelConfig.TOKENIZER_PATH
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
        self.confidence_threshold = confidence_threshold
        
        self.model = None
        self.tokenizer = None
        self.is_loaded = False
    
    def load_model(self) -> bool:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞"""
        try:
            if not os.path.exists(self.model_path):
                print(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {self.model_path}")
                return False
            
            if not os.path.exists(self.tokenizer_path):
                print(f"‚ö†Ô∏è –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω: {self.tokenizer_path}")
                return False
            
            self.tokenizer = SimpleTokenizer.load(self.tokenizer_path)
            checkpoint = torch.load(self.model_path, map_location=self.device)
            
            encoder = Encoder(
                vocab_size=checkpoint['vocab_size'],
                embedding_dim=checkpoint['embedding_dim'],
                hidden_size=checkpoint['hidden_size'],
                num_layers=checkpoint['num_layers'],
                dropout=0.0
            )
            
            decoder = Decoder(
                vocab_size=checkpoint['vocab_size'],
                embedding_dim=checkpoint['embedding_dim'],
                hidden_size=checkpoint['hidden_size'],
                num_layers=checkpoint['num_layers'],
                dropout=0.0,
                use_attention=True
            )
            
            self.model = Seq2Seq(encoder, decoder, self.device)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.model = self.model.to(self.device)
            self.model.eval()
            
            self.is_loaded = True
            print(f"‚úÖ ML –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ ({self.device})")
            
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            return False
    
    def generate_answer(self, question: str, max_length: int = 100) -> Tuple[Optional[str], float]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å
        
        Args:
            question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞
        
        Returns:
            (answer, confidence): –û—Ç–≤–µ—Ç –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (0-1)
        """
        if not self.is_loaded:
            return None, 0.0
        
        try:
            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
            question_indices = self.tokenizer.encode(
                question,
                max_length=ModelConfig.MAX_SEQ_LENGTH,
                add_sos=False,
                add_eos=True
            )
            
            # –¢–µ–Ω–∑–æ—Ä—ã
            question_tensor = torch.LongTensor(question_indices).unsqueeze(0).to(self.device)
            question_length = torch.LongTensor([sum(1 for idx in question_indices if idx != 0)])
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
            with torch.no_grad():
                generated_tokens = self.model.generate(
                    question_tensor,
                    question_length,
                    max_length=max_length,
                    sos_token=2,
                    eos_token=3
                )
            
            # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
            answer = self.tokenizer.decode(
                generated_tokens[0].cpu().tolist(),
                skip_special=True
            )
            
            # –£–ø—Ä–æ—â—ë–Ω–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (–ø–æ –¥–ª–∏–Ω–µ)
            answer_length = len(answer.split())
            confidence = min(answer_length / 10.0, 1.0) if answer_length > 3 else 0.3
            
            return answer, confidence
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            return None, 0.0
    
    def should_use_ml(self, confidence: float) -> bool:
        """–†–µ—à–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ ML –æ—Ç–≤–µ—Ç"""
        return confidence >= self.confidence_threshold


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
ml_inference = MLModelInference(confidence_threshold=0.6)


def initialize_ml_model() -> bool:
    """
    –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ML –º–æ–¥–µ–ª–∏ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ –±–æ—Ç–∞
    
    Returns:
        True –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞
    """
    return ml_inference.load_model()


def get_ml_answer(question: str) -> Tuple[Optional[str], bool]:
    """
    –ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –æ—Ç ML –º–æ–¥–µ–ª–∏
    
    Args:
        question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    
    Returns:
        (answer, use_ml): –û—Ç–≤–µ—Ç –∏ —Ñ–ª–∞–≥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ –µ–≥–æ
                         (None, False) –µ—Å–ª–∏ fallback –Ω–∞ API
    """
    if not ml_inference.is_loaded:
        return None, False
    
    answer, confidence = ml_inference.generate_answer(question)
    
    if answer and ml_inference.should_use_ml(confidence):
        return answer, True
    
    return None, False


if __name__ == "__main__":
    """–¢–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
    print("\n" + "=" * 60)
    print("–¢–ï–°–¢ ML INFERENCE")
    print("=" * 60)
    
    success = initialize_ml_model()
    
    if success:
        test_questions = [
            "–°–∫–æ–ª—å–∫–æ —Å—Ç–æ–∏—Ç –æ–±—É—á–µ–Ω–∏–µ?",
            "–ö–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω—É–∂–Ω—ã?",
            "–ï—Å—Ç—å –ª–∏ –±—é–¥–∂–µ—Ç–Ω—ã–µ –º–µ—Å—Ç–∞?"
        ]
        
        print("\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ:")
        for q in test_questions:
            print(f"\n‚ùì {q}")
            answer, use_ml = get_ml_answer(q)
            
            if use_ml:
                print(f"‚úÖ ML: {answer}")
            else:
                print(f"‚ö†Ô∏è Fallback –Ω–∞ API")
    else:
        print("\n‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    
    print("\n" + "=" * 60)
