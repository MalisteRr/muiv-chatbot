"""
–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è (—Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞) –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

–°–æ–∑–¥–∞—ë—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —á–∏—Å–µ–ª.
"""

import sys
import os
import json
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø—Ä–æ–µ–∫—Ç–∞ –≤ –ø—É—Ç—å
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from ml.models.tokenizer import SimpleTokenizer
from ml.models.config import ModelConfig


def build_vocabulary():
    """
    –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
    """
    print("\n" + "=" * 70)
    print("–ü–û–°–¢–†–û–ï–ù–ò–ï –°–õ–û–í–ê–†–Ø (–¢–û–ö–ï–ù–ò–ó–ê–¢–û–†–ê)")
    print("=" * 70)
    
    # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –¥–∞–Ω–Ω—ã—Ö
    print(f"\nüìÇ –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    
    if not os.path.exists(ModelConfig.DATA_PATH):
        print(f"‚ùå –î–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {ModelConfig.DATA_PATH}")
        print(f"\nüí° –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ:")
        print(f"   python scripts/prepare_dataset.py")
        return
    
    print(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç –Ω–∞–π–¥–µ–Ω: {ModelConfig.DATA_PATH}")
    
    # 2. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print(f"\nüì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    
    with open(ModelConfig.DATA_PATH, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(data)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    # 3. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤
    print(f"\nüìù –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤...")
    
    texts = []
    for item in data:
        texts.append(item['question'])
        texts.append(item['answer'])
    
    print(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤")
    
    # 4. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è
    print(f"\nüî® –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è...")
    print(f"   –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {ModelConfig.VOCAB_SIZE}")
    
    tokenizer = SimpleTokenizer(vocab_size=ModelConfig.VOCAB_SIZE)
    tokenizer.build_vocab(texts)
    
    actual_vocab_size = tokenizer.get_vocab_size()
    print(f"‚úÖ –°–ª–æ–≤–∞—Ä—å –ø–æ—Å—Ç—Ä–æ–µ–Ω: {actual_vocab_size} —Ç–æ–∫–µ–Ω–æ–≤")
    
    # 5. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
    print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
    
    # –°–æ–∑–¥–∞—ë–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    os.makedirs(os.path.dirname(ModelConfig.TOKENIZER_PATH), exist_ok=True)
    
    tokenizer.save(ModelConfig.TOKENIZER_PATH)
    
    print(f"‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {ModelConfig.TOKENIZER_PATH}")
    
    # 6. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print("\n" + "=" * 70)
    print("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –°–õ–û–í–ê–†–Ø")
    print("=" * 70)
    print(f"üìä –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {actual_vocab_size}")
    print(f"üìä –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã:")
    print(f"   <PAD>: {tokenizer.word2idx.get('<PAD>', '–Ω–µ –Ω–∞–π–¥–µ–Ω')}")
    print(f"   <UNK>: {tokenizer.word2idx.get('<UNK>', '–Ω–µ –Ω–∞–π–¥–µ–Ω')}")
    print(f"   <SOS>: {tokenizer.word2idx.get('<SOS>', '–Ω–µ –Ω–∞–π–¥–µ–Ω')}")
    print(f"   <EOS>: {tokenizer.word2idx.get('<EOS>', '–Ω–µ –Ω–∞–π–¥–µ–Ω')}")
    
    # –¢–æ–ø —Å–ª–æ–≤
    print(f"\nüìà –¢–æ–ø-20 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö —Å–ª–æ–≤:")
    
    # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø —Å–ª–æ–≤–∞ (–∫—Ä–æ–º–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤)
    word_freq = {}
    for text in texts:
        words = tokenizer.tokenize(text)
        for word in words:
            if word not in ['<PAD>', '<UNK>', '<SOS>', '<EOS>']:
                word_freq[word] = word_freq.get(word, 0) + 1
    
    top_words = sorted(word_freq.items(), key=lambda x: -x[1])[:20]
    
    for i, (word, freq) in enumerate(top_words, 1):
        print(f"   {i:2d}. {word:20s} ({freq:4d} —Ä–∞–∑)")
    
    # –ü—Ä–∏–º–µ—Ä—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
    print(f"\nüìù –ü—Ä–∏–º–µ—Ä—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏:")
    
    test_sentences = [
        "–°–∫–æ–ª—å–∫–æ —Å—Ç–æ–∏—Ç –æ–±—É—á–µ–Ω–∏–µ?",
        "–ö–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω—É–∂–Ω—ã –¥–ª—è –ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏—è?",
        "–ï—Å—Ç—å –ª–∏ –±—é–¥–∂–µ—Ç–Ω—ã–µ –º–µ—Å—Ç–∞?"
    ]
    
    for sent in test_sentences:
        tokens = tokenizer.encode(sent, add_sos=True, add_eos=True)
        decoded = tokenizer.decode(tokens, skip_special=True)
        
        print(f"\n   –ò—Å—Ö–æ–¥–Ω—ã–π: {sent}")
        print(f"   –¢–æ–∫–µ–Ω—ã: {tokens[:15]}..." if len(tokens) > 15 else f"   –¢–æ–∫–µ–Ω—ã: {tokens}")
        print(f"   –î–ª–∏–Ω–∞: {len(tokens)}")
        print(f"   –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π: {decoded}")
    
    print("\n" + "=" * 70)
    print("‚úÖ –ü–û–°–¢–†–û–ï–ù–ò–ï –°–õ–û–í–ê–†–Ø –ó–ê–í–ï–†–®–ï–ù–û")
    print("=" * 70)
    print(f"\nüìå –°–ª–µ–¥—É—é—â–∏–π —à–∞–≥:")
    print(f"   python scripts/train_model.py")
    print("=" * 70 + "\n")


if __name__ == "__main__":
    build_vocabulary()
