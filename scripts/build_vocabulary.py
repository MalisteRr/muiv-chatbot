"""
–ß—Ç–æ –¥–µ–ª–∞–µ—Ç —Å–∫—Ä–∏–ø—Ç:
1. –ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
2. –ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ –≤–æ–ø—Ä–æ—Å—ã –∏ –æ—Ç–≤–µ—Ç—ã
3. –°—Ç—Ä–æ–∏—Ç —Å–ª–æ–≤–∞—Ä—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤
4. –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
"""

import json
import os
import sys

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

from ml.models.tokenizer import SimpleTokenizer
from ml.models.config import ModelConfig


def load_dataset():
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
    
    Returns:
        –°–ø–∏—Å–æ–∫ –ø–∞—Ä –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç
    """
    try:
        with open(ModelConfig.DATA_PATH, 'r', encoding='utf-8') as f:
            data = json.load(f)
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω –¥–∞—Ç–∞—Å–µ—Ç: {len(data)} –ø–∞—Ä")
        return data
    except FileNotFoundError:
        print(f"‚ùå –§–∞–π–ª –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω: {ModelConfig.DATA_PATH}")
        print("   –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ: python scripts/prepare_dataset.py")
        return []


def extract_texts(dataset):
    """
    –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Å–µ—Ö —Ç–µ–∫—Å—Ç–æ–≤ (–≤–æ–ø—Ä–æ—Å—ã + –æ—Ç–≤–µ—Ç—ã) –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞
    
    Args:
        dataset: –°–ø–∏—Å–æ–∫ –ø–∞—Ä –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç
        
    Returns:
        –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Ç–µ–∫—Å—Ç–æ–≤
    """
    all_texts = []
    
    for item in dataset:
        all_texts.append(item['question'])
        all_texts.append(item['answer'])
    
    print(f"üìä –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(all_texts)} —Ç–µ–∫—Å—Ç–æ–≤:")
    print(f"   ‚Ä¢ –í–æ–ø—Ä–æ—Å–æ–≤: {len(dataset)}")
    print(f"   ‚Ä¢ –û—Ç–≤–µ—Ç–æ–≤: {len(dataset)}")
    
    return all_texts


def print_vocabulary_stats(tokenizer):
    """
    –í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –ø–æ—Å—Ç—Ä–æ–µ–Ω–Ω–æ–º—É —Å–ª–æ–≤–∞—Ä—é
    
    Args:
        tokenizer: –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ —Å–ª–æ–≤–∞—Ä—ë–º
    """
    print("\n" + "=" * 60)
    print("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –°–õ–û–í–ê–†–Ø")
    print("=" * 60)
    
    print(f"üìö –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {tokenizer.get_vocab_size()} —Å–ª–æ–≤")
    print(f"   ‚Ä¢ –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã: 4")
    print(f"   ‚Ä¢ –û–±—ã—á–Ω—ã–µ —Å–ª–æ–≤–∞: {tokenizer.get_vocab_size() - 4}")
    
    # –¢–æ–ø-20 —Å–∞–º—ã—Ö —á–∞—Å—Ç–æ—Ç–Ω—ã—Ö —Å–ª–æ–≤
    print(f"\nüîù –¢–æ–ø-20 —Å–∞–º—ã—Ö —á–∞—Å—Ç–æ—Ç–Ω—ã—Ö —Å–ª–æ–≤:")
    top_words = tokenizer.word_count.most_common(20)
    for idx, (word, count) in enumerate(top_words, 1):
        print(f"   {idx:2d}. {word:15s} - {count:4d} —Ä–∞–∑")
    
    # –ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
    print(f"\nüß™ –ü—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è:")
    test_texts = [
        "–°–∫–æ–ª—å–∫–æ —Å—Ç–æ–∏—Ç –æ–±—É—á–µ–Ω–∏–µ?",
        "–ï—Å—Ç—å –ª–∏ –±—é–¥–∂–µ—Ç–Ω—ã–µ –º–µ—Å—Ç–∞?",
        "–ö–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω—É–∂–Ω—ã?"
    ]
    
    for text in test_texts:
        encoded = tokenizer.encode(text, max_length=20)
        decoded = tokenizer.decode(encoded)
        print(f"\n   –ò—Å—Ö–æ–¥–Ω—ã–π: {text}")
        print(f"   –ò–Ω–¥–µ–∫—Å—ã: {encoded[:10]}... (–ø–µ—Ä–≤—ã–µ 10)")
        print(f"   –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π: {decoded}")
    
    print("=" * 60)


def main():
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
    """
    print("\n" + "=" * 60)
    print("–ü–û–°–¢–†–û–ï–ù–ò–ï –°–õ–û–í–ê–†–Ø –î–õ–Ø –ú–û–î–ï–õ–ò")
    print("=" * 60)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
    dataset = load_dataset()
    if not dataset:
        print("\n‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç. –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ.")
        return
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç—ã
    all_texts = extract_texts(dataset)
    
    # –°–æ–∑–¥–∞—ë–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    print(f"\nüî® –°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ (vocab_size={ModelConfig.VOCAB_SIZE})...")
    tokenizer = SimpleTokenizer(vocab_size=ModelConfig.VOCAB_SIZE)
    
    # –°—Ç—Ä–æ–∏–º —Å–ª–æ–≤–∞—Ä—å
    tokenizer.build_vocab(all_texts)
    
    # –°–æ–∑–¥–∞—ë–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    os.makedirs(os.path.dirname(ModelConfig.TOKENIZER_PATH), exist_ok=True)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    tokenizer.save(ModelConfig.TOKENIZER_PATH)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print_vocabulary_stats(tokenizer)
    
    print(f"\n‚úÖ –°–ª–æ–≤–∞—Ä—å —É—Å–ø–µ—à–Ω–æ –ø–æ—Å—Ç—Ä–æ–µ–Ω –∏ —Å–æ—Ö—Ä–∞–Ω—ë–Ω!")
    print(f"üìÅ –§–∞–π–ª: {ModelConfig.TOKENIZER_PATH}")
    print(f"üìö –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {tokenizer.get_vocab_size()} —Å–ª–æ–≤")
    print(f"\nüí° –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª—å!")


if __name__ == "__main__":
    main()
