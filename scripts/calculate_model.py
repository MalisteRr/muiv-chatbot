"""
–°–∫—Ä–∏–ø—Ç –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏

–í—ã—á–∏—Å–ª—è–µ—Ç BLEU score –∏ Accuracy –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ.
"""

import sys
import os
import torch
from typing import List
import json

sys.path.append(os.path.dirname(os.path.dirname(__file__)))

from ml.models import (
    Seq2Seq,
    Encoder,
    Decoder,
    SimpleTokenizer,
    ModelConfig
)


def compute_bleu(reference: List[str], hypothesis: List[str]) -> float:
    """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ BLEU score (—É–ø—Ä–æ—â—ë–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è BLEU-1)"""
    ref_words = set(reference)
    hyp_words = hypothesis
    
    if not hyp_words:
        return 0.0
    
    matches = sum(1 for word in hyp_words if word in ref_words)
    precision = matches / len(hyp_words)
    
    return precision


def compute_exact_match(reference: str, hypothesis: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è"""
    ref_normalized = ' '.join(reference.lower().split())
    hyp_normalized = ' '.join(hypothesis.lower().split())
    
    return ref_normalized == hyp_normalized


def evaluate_model(
    model_path: str,
    tokenizer_path: str,
    test_data_path: str,
    device: str = 'cpu'
):
    """
    –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    
    Args:
        model_path: –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏
        tokenizer_path: –ü—É—Ç—å –∫ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—É
        test_data_path: –ü—É—Ç—å –∫ —Ç–µ—Å—Ç–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º
        device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
    
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    """
    print("\n" + "=" * 70)
    print("–û–¶–ï–ù–ö–ê –ö–ê–ß–ï–°–¢–í–ê –ú–û–î–ï–õ–ò")
    print("=" * 70)
    
    # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    print(f"\nüìö –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
    tokenizer = SimpleTokenizer.load(tokenizer_path)
    
    # 2. –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    print(f"üèóÔ∏è –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
    checkpoint = torch.load(model_path, map_location=device)
    
    encoder = Encoder(
        vocab_size=checkpoint['vocab_size'],
        embedding_dim=checkpoint['embedding_dim'],
        hidden_size=checkpoint['hidden_size'],
        num_layers=checkpoint['num_layers'],
        dropout=0.0
    )
    
    decoder = Decoder(
        vocab_size=checkpoint['vocab_size'],
        embedding_dim=checkpoint['embedding_dim'],
        hidden_size=checkpoint['hidden_size'],
        num_layers=checkpoint['num_layers'],
        dropout=0.0,
        use_attention=True
    )
    
    model = Seq2Seq(encoder, decoder, device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model = model.to(device)
    model.eval()
    
    print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    
    # 3. –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    print(f"\nüì¶ –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    with open(test_data_path, 'r', encoding='utf-8') as f:
        test_data = json.load(f)
    
    print(f"   –¢–µ—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {len(test_data)}")
    
    # 4. –û—Ü–µ–Ω–∫–∞
    print(f"\nüß™ –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏...")
    
    bleu_scores = []
    exact_matches = 0
    total = 0
    
    for idx, item in enumerate(test_data):
        question = item['question']
        reference_answer = item['answer']
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        question_indices = tokenizer.encode(
            question,
            max_length=ModelConfig.MAX_SEQ_LENGTH,
            add_sos=False,
            add_eos=True
        )
        
        question_tensor = torch.LongTensor(question_indices).unsqueeze(0).to(device)
        question_length = torch.LongTensor([sum(1 for idx in question_indices if idx != 0)])
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
        with torch.no_grad():
            generated_tokens = model.generate(
                question_tensor,
                question_length,
                max_length=100,
                sos_token=2,
                eos_token=3
            )
        
        # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
        hypothesis_answer = tokenizer.decode(
            generated_tokens[0].cpu().tolist(),
            skip_special=True
        )
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        ref_words = reference_answer.lower().split()
        hyp_words = hypothesis_answer.lower().split()
        
        bleu = compute_bleu(ref_words, hyp_words)
        bleu_scores.append(bleu)
        
        if compute_exact_match(reference_answer, hypothesis_answer):
            exact_matches += 1
        
        total += 1
        
        # –ü—Ä–æ–≥—Ä–µ—Å—Å
        if (idx + 1) % 10 == 0:
            print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {idx + 1}/{len(test_data)}...")
    
    # 5. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
    avg_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0.0
    accuracy = exact_matches / total if total > 0 else 0.0
    
    print("\n" + "=" * 70)
    print("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–¶–ï–ù–ö–ò")
    print("=" * 70)
    print(f"üìä –ú–µ—Ç—Ä–∏–∫–∏ –Ω–∞ {total} –ø—Ä–∏–º–µ—Ä–∞—Ö:")
    print(f"   BLEU Score: {avg_bleu:.4f} ({avg_bleu * 100:.2f}%)")
    print(f"   Accuracy: {accuracy:.4f} ({accuracy * 100:.2f}%)")
    print(f"   Exact Matches: {exact_matches}/{total}")
    
    # –¶–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
    target_bleu = ModelConfig.TARGET_BLEU
    target_acc = ModelConfig.TARGET_ACCURACY
    
    print(f"\nüéØ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å —Ü–µ–ª–µ–≤—ã–º–∏:")
    print(f"   BLEU: {avg_bleu:.4f} vs {target_bleu:.4f} {'‚úÖ' if avg_bleu >= target_bleu else '‚ö†Ô∏è'}")
    print(f"   Accuracy: {accuracy:.4f} vs {target_acc:.4f} {'‚úÖ' if accuracy >= target_acc else '‚ö†Ô∏è'}")
    
    # –ü—Ä–∏–º–µ—Ä—ã
    print(f"\nüìù –ü—Ä–∏–º–µ—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:")
    for i in range(min(3, len(test_data))):
        item = test_data[i]
        question = item['question']
        reference = item['answer']
        
        question_indices = tokenizer.encode(question, max_length=ModelConfig.MAX_SEQ_LENGTH, add_sos=False, add_eos=True)
        question_tensor = torch.LongTensor(question_indices).unsqueeze(0).to(device)
        question_length = torch.LongTensor([sum(1 for idx in question_indices if idx != 0)])
        
        with torch.no_grad():
            generated_tokens = model.generate(question_tensor, question_length, max_length=100, sos_token=2, eos_token=3)
        
        hypothesis = tokenizer.decode(generated_tokens[0].cpu().tolist(), skip_special=True)
        
        print(f"\n   {i+1}. –í–æ–ø—Ä–æ—Å: {question}")
        print(f"      –≠—Ç–∞–ª–æ–Ω: {reference}")
        print(f"      –ú–æ–¥–µ–ª—å: {hypothesis}")
    
    print("\n" + "=" * 70 + "\n")
    
    return {
        'bleu': avg_bleu,
        'accuracy': accuracy,
        'total': total,
        'exact_matches': exact_matches
    }


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    
    # –ü—É—Ç–∏
    model_path = ModelConfig.MODEL_SAVE_PATH
    tokenizer_path = ModelConfig.TOKENIZER_PATH
    test_data_path = os.path.join(
        os.path.dirname(ModelConfig.DATA_PATH),
        'test_data.json'
    )
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∏
    if not os.path.exists(model_path):
        print(f"‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {model_path}")
        print("   –û–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å: python scripts/train_model.py")
        return
    
    if not os.path.exists(test_data_path):
        print(f"‚ùå –¢–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {test_data_path}")
        print("   –ó–∞–ø—É—Å—Ç–∏—Ç–µ: python scripts/prepare_dataset.py")
        return
    
    # –û—Ü–µ–Ω–∫–∞
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    results = evaluate_model(model_path, tokenizer_path, test_data_path, device)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    results_path = os.path.join(ModelConfig.CHECKPOINT_DIR, 'evaluation_results.json')
    os.makedirs(os.path.dirname(results_path), exist_ok=True)
    
    with open(results_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {results_path}")


if __name__ == "__main__":
    main()
