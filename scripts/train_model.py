"""
–°–∫—Ä–∏–ø—Ç –æ–±—É—á–µ–Ω–∏—è Seq2Seq –º–æ–¥–µ–ª–∏

–ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –∞–±–∏—Ç—É—Ä–∏–µ–Ω—Ç–æ–≤.

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python scripts/train_model.py
"""

import sys
import os
import torch

sys.path.append(os.path.dirname(os.path.dirname(__file__)))

from ml.models import (
    create_seq2seq_model,
    create_dataloaders,
    create_trainer,
    SimpleTokenizer,
    ModelConfig,
    split_dataset
)


def check_prerequisites():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Ñ–∞–π–ª–æ–≤"""
    print(f"\nüìÅ –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    
    if not os.path.exists(ModelConfig.DATA_PATH):
        print(f"‚ùå –î–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {ModelConfig.DATA_PATH}")
        print("   –ó–∞–ø—É—Å—Ç–∏—Ç–µ: python scripts/prepare_dataset.py")
        return False
    
    if not os.path.exists(ModelConfig.TOKENIZER_PATH):
        print(f"‚ùå –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω: {ModelConfig.TOKENIZER_PATH}")
        print("   –ó–∞–ø—É—Å—Ç–∏—Ç–µ: python scripts/build_vocabulary.py")
        return False
    
    print(f"‚úÖ –í—Å–µ —Ñ–∞–π–ª—ã –Ω–∞–π–¥–µ–Ω—ã")
    return True


def prepare_data_splits():
    """–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–∞ train/val/test"""
    train_path = os.path.join(os.path.dirname(ModelConfig.DATA_PATH), 'train_data.json')
    val_path = os.path.join(os.path.dirname(ModelConfig.DATA_PATH), 'val_data.json')
    
    if not os.path.exists(train_path):
        print(f"\n‚úÇÔ∏è –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
        train_path, val_path, _ = split_dataset(
            ModelConfig.DATA_PATH,
            train_ratio=0.8,
            val_ratio=0.1,
            test_ratio=0.1,
            save_splits=True
        )
    else:
        print(f"\n‚úÖ –î–∞—Ç–∞—Å–µ—Ç —É–∂–µ —Ä–∞–∑–¥–µ–ª—ë–Ω")
    
    return train_path, val_path


def load_tokenizer_and_data(train_path, val_path):
    """–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ DataLoader'–æ–≤"""
    print(f"\nüìö –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
    tokenizer = SimpleTokenizer.load(ModelConfig.TOKENIZER_PATH)
    vocab_size = tokenizer.get_vocab_size()
    print(f"   –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {vocab_size}")
    
    print(f"\nüì¶ –°–æ–∑–¥–∞–Ω–∏–µ DataLoader'–æ–≤...")
    train_loader, val_loader = create_dataloaders(
        train_path=train_path,
        val_path=val_path,
        tokenizer=tokenizer,
        batch_size=ModelConfig.BATCH_SIZE,
        max_length=ModelConfig.MAX_SEQ_LENGTH,
        num_workers=0
    )
    
    return train_loader, val_loader, vocab_size


def create_model_and_trainer(vocab_size, device):
    """–°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ trainer'–∞"""
    print(f"\nüèóÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    model = create_seq2seq_model(
        vocab_size=vocab_size,
        embedding_dim=ModelConfig.EMBEDDING_DIM,
        hidden_size=ModelConfig.HIDDEN_SIZE,
        num_layers=ModelConfig.NUM_LAYERS,
        dropout=ModelConfig.DROPOUT,
        use_attention=True,
        device=device
    )
    
    print(f"\nüéì –°–æ–∑–¥–∞–Ω–∏–µ Trainer...")
    trainer = create_trainer(
        model=model,
        learning_rate=ModelConfig.LEARNING_RATE,
        device=device
    )
    
    return model, trainer


def train_model(model, trainer, train_loader, val_loader):
    """
    –ó–∞–ø—É—Å–∫ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è
    """
    print(f"\nüöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è...")
    print(f"   –≠–ø–æ—Ö: {ModelConfig.NUM_EPOCHS}")
    print(f"   Batch size: {ModelConfig.BATCH_SIZE}")
    print(f"   Learning rate: {ModelConfig.LEARNING_RATE}")
    
    # –°–æ–∑–¥–∞—ë–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤
    os.makedirs(ModelConfig.CHECKPOINT_DIR, exist_ok=True)
    
    # –û–±—É—á–µ–Ω–∏–µ
    trainer.train(
        train_loader=train_loader,
        val_loader=val_loader,
        num_epochs=ModelConfig.NUM_EPOCHS,
        teacher_forcing_ratio=ModelConfig.TEACHER_FORCING_RATIO,
        save_dir=ModelConfig.CHECKPOINT_DIR,
        early_stopping_patience=3
    )
    
    return trainer


def save_final_model(model, trainer, vocab_size):
    """
    –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
    """
    print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏...")
    final_model_path = ModelConfig.MODEL_SAVE_PATH
    os.makedirs(os.path.dirname(final_model_path), exist_ok=True)
    
    torch.save({
        'model_state_dict': model.state_dict(),
        'vocab_size': vocab_size,
        'embedding_dim': ModelConfig.EMBEDDING_DIM,
        'hidden_size': ModelConfig.HIDDEN_SIZE,
        'num_layers': ModelConfig.NUM_LAYERS,
        'dropout': ModelConfig.DROPOUT,
        'train_losses': trainer.train_losses,
        'val_losses': trainer.val_losses
    }, final_model_path)
    
    print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {final_model_path}")
    
    return final_model_path


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("\n" + "=" * 70)
    print("–û–ë–£–ß–ï–ù–ò–ï SEQ2SEQ –ú–û–î–ï–õ–ò –î–õ–Ø –ß–ê–¢-–ë–û–¢–ê –ú–£–ò–í")
    print("=" * 70)
    
    # 1. –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"\nüñ•Ô∏è –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    if device == 'cuda':
        print(f"   GPU: {torch.cuda.get_device_name(0)}")
    
    # 2. –ü—Ä–æ–≤–µ—Ä–∫–∏
    if not check_prerequisites():
        return
    
    # 3. –î–∞–Ω–Ω—ã–µ
    train_path, val_path = prepare_data_splits()
    train_loader, val_loader, vocab_size = load_tokenizer_and_data(train_path, val_path)
    
    # 4. –ú–æ–¥–µ–ª—å
    model, trainer = create_model_and_trainer(vocab_size, device)
    
    # 5. –û–±—É—á–µ–Ω–∏–µ
    trainer = train_model(model, trainer, train_loader, val_loader)
    
    # 6. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    final_path = save_final_model(model, trainer, vocab_size)
    
    # 7. –ò—Ç–æ–≥–∏
    print("\n" + "=" * 70)
    print("–û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û")
    print("=" * 70)
    print(f"‚úÖ –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å: {final_path}")
    print(f"‚úÖ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {os.path.join(ModelConfig.CHECKPOINT_DIR, 'best_model.pt')}")
    print(f"üìä –õ—É—á—à–∏–π Val Loss: {trainer.best_val_loss:.4f}")
    print("=" * 70 + "\n")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è –û–±—É—á–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ")
    except Exception as e:
        print(f"\n\n‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
