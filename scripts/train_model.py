"""
–°–∫—Ä–∏–ø—Ç –æ–±—É—á–µ–Ω–∏—è Seq2Seq –º–æ–¥–µ–ª–∏

–ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –∞–±–∏—Ç—É—Ä–∏–µ–Ω—Ç–æ–≤.

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python scripts/train_model.py
"""

import sys
import os
import torch

sys.path.append(os.path.dirname(os.path.dirname(__file__)))

from ml.models import (
    create_seq2seq_model,
    create_dataloaders,
    create_trainer,
    SimpleTokenizer,
    ModelConfig,
    split_dataset
)


def check_prerequisites():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Ñ–∞–π–ª–æ–≤"""
    print(f"\nüìÅ –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    
    if not os.path.exists(ModelConfig.DATA_PATH):
        print(f"‚ùå –î–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {ModelConfig.DATA_PATH}")
        print("   –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–Ω–∞—á–∞–ª–∞:")
        print("   1. python scripts/prepare_dataset.py")
        print("   2. python scripts/build_vocabulary.py")
        return False
    
    if not os.path.exists(ModelConfig.TOKENIZER_PATH):
        print(f"‚ùå –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω: {ModelConfig.TOKENIZER_PATH}")
        print("   –ó–∞–ø—É—Å—Ç–∏—Ç–µ: python scripts/build_vocabulary.py")
        return False
    
    print(f"‚úÖ –í—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ñ–∞–π–ª—ã –Ω–∞–π–¥–µ–Ω—ã")
    return True


def prepare_data_splits():
    """–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–∞ train/val/test"""
    print(f"\n‚úÇÔ∏è –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö...")
    
    train_path = os.path.join(os.path.dirname(ModelConfig.DATA_PATH), 'train_data.json')
    val_path = os.path.join(os.path.dirname(ModelConfig.DATA_PATH), 'val_data.json')
    test_path = os.path.join(os.path.dirname(ModelConfig.DATA_PATH), 'test_data.json')
    
    if os.path.exists(train_path) and os.path.exists(val_path):
        print(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç —É–∂–µ —Ä–∞–∑–¥–µ–ª—ë–Ω")
        return train_path, val_path, test_path
    
    print(f"   –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
    train_path, val_path, test_path = split_dataset(
        ModelConfig.DATA_PATH,
        train_ratio=0.8,
        val_ratio=0.1,
        test_ratio=0.1,
        save_splits=True
    )
    
    return train_path, val_path, test_path


def load_tokenizer_and_data(train_path, val_path):
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ DataLoader'–æ–≤
    
    Returns:
        (tokenizer, train_loader, val_loader, vocab_size)
    """
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    print(f"\nüìö –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
    tokenizer = SimpleTokenizer.load(ModelConfig.TOKENIZER_PATH)
    vocab_size = tokenizer.get_vocab_size()
    print(f"   –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {vocab_size}")
    
    # –°–æ–∑–¥–∞—ë–º DataLoader'—ã
    print(f"\nüì¶ –°–æ–∑–¥–∞–Ω–∏–µ DataLoader'–æ–≤...")
    train_loader, val_loader = create_dataloaders(
        train_path=train_path,
        val_path=val_path,
        tokenizer=tokenizer,
        batch_size=ModelConfig.BATCH_SIZE,
        max_length=ModelConfig.MAX_SEQ_LENGTH,
        num_workers=0
    )
    
    return tokenizer, train_loader, val_loader, vocab_size


def create_model_and_trainer(vocab_size, device):
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ trainer'–∞
    
    Returns:
        (model, trainer)
    """
    # –°–æ–∑–¥–∞—ë–º –º–æ–¥–µ–ª—å
    print(f"\nüèóÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    model = create_seq2seq_model(
        vocab_size=vocab_size,
        embedding_dim=ModelConfig.EMBEDDING_DIM,
        hidden_size=ModelConfig.HIDDEN_SIZE,
        num_layers=ModelConfig.NUM_LAYERS,
        dropout=ModelConfig.DROPOUT,
        use_attention=True,
        device=device
    )
    
    # –°–æ–∑–¥–∞—ë–º Trainer
    print(f"\nüéì –°–æ–∑–¥–∞–Ω–∏–µ Trainer...")
    trainer = create_trainer(
        model=model,
        learning_rate=ModelConfig.LEARNING_RATE,
        device=device
    )
    
    return model, trainer


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è"""
    print("\n" + "=" * 70)
    print("–û–ë–£–ß–ï–ù–ò–ï SEQ2SEQ –ú–û–î–ï–õ–ò –î–õ–Ø –ß–ê–¢-–ë–û–¢–ê –ú–£–ò–í")
    print("=" * 70)
    
    # 1. –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"\nüñ•Ô∏è –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    if device == 'cuda':
        print(f"   GPU: {torch.cuda.get_device_name(0)}")
    
    # 2. –ü—Ä–æ–≤–µ—Ä–∫–∏
    if not check_prerequisites():
        return
    
    # 3. –î–∞–Ω–Ω—ã–µ
    train_path, val_path, test_path = prepare_data_splits()
    
    # 4. –ó–∞–≥—Ä—É–∑–∫–∞
    tokenizer, train_loader, val_loader, vocab_size = load_tokenizer_and_data(
        train_path, val_path
    )
    
    # 5. –ú–æ–¥–µ–ª—å
    model, trainer = create_model_and_trainer(vocab_size, device)
    
    print("\n‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    print("   –ì–æ—Ç–æ–≤–æ –∫ –æ–±—É—á–µ–Ω–∏—é!")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"\n\n‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
