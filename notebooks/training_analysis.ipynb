{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä –ê–Ω–∞–ª–∏–∑ –æ–±—É—á–µ–Ω–∏—è Seq2Seq –º–æ–¥–µ–ª–∏\n",
    "\n",
    "–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è —á–∞—Ç-–±–æ—Ç–∞ –ú–£–ò–í"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# –°—Ç–∏–ª—å –≥—Ä–∞—Ñ–∏–∫–æ–≤\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏\n",
    "model_path = '../ml/models/chatbot_model.pt'\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º checkpoint\n",
    "checkpoint = torch.load(model_path, map_location='cpu')\n",
    "\n",
    "# –ò–∑–≤–ª–µ–∫–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –æ–±—É—á–µ–Ω–∏—è\n",
    "train_losses = checkpoint.get('train_losses', [])\n",
    "val_losses = checkpoint.get('val_losses', [])\n",
    "\n",
    "print(f\"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –æ–±—É—á–µ–Ω–∏—è\")\n",
    "print(f\"   –≠–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è: {len(train_losses)}\")\n",
    "print(f\"   –§–∏–Ω–∞–ª—å–Ω—ã–π Train Loss: {train_losses[-1]:.4f}\")\n",
    "if val_losses:\n",
    "    print(f\"   –§–∏–Ω–∞–ª—å–Ω—ã–π Val Loss: {val_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. –ì—Ä–∞—Ñ–∏–∫ Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(epochs, train_losses, 'b-', label='Train Loss', linewidth=2)\n",
    "if val_losses:\n",
    "    plt.plot(epochs, val_losses, 'r-', label='Validation Loss', linewidth=2)\n",
    "\n",
    "plt.xlabel('–≠–ø–æ—Ö–∞', fontsize=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "plt.title('–î–∏–Ω–∞–º–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏', fontsize=16, fontweight='bold')\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_loss.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"üíæ –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω: training_loss.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ–∑–¥–∞—ë–º DataFrame\n",
    "df = pd.DataFrame({\n",
    "    '–≠–ø–æ—Ö–∞': epochs,\n",
    "    'Train Loss': train_losses,\n",
    "    'Val Loss': val_losses if val_losses else [None] * len(train_losses)\n",
    "})\n",
    "\n",
    "print(\"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è:\")\n",
    "print(df.describe())\n",
    "\n",
    "# –õ—É—á—à–∞—è —ç–ø–æ—Ö–∞\n",
    "if val_losses:\n",
    "    best_epoch = np.argmin(val_losses) + 1\n",
    "    best_val_loss = min(val_losses)\n",
    "    print(f\"\\n‚úÖ –õ—É—á—à–∞—è —ç–ø–æ—Ö–∞: {best_epoch}\")\n",
    "    print(f\"   Val Loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏:\")\n",
    "print(f\"   Vocab size: {checkpoint['vocab_size']:,}\")\n",
    "print(f\"   Embedding dim: {checkpoint['embedding_dim']}\")\n",
    "print(f\"   Hidden size: {checkpoint['hidden_size']}\")\n",
    "print(f\"   Num layers: {checkpoint['num_layers']}\")\n",
    "print(f\"   Dropout: {checkpoint['dropout']}\")\n",
    "\n",
    "# –ü–æ–¥—Å—á—ë—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "total_params = sum(p.numel() for p in checkpoint['model_state_dict'].values())\n",
    "print(f\"\\nüìä –í—Å–µ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {total_params:,}\")\n",
    "print(f\"   –†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏: {total_params * 4 / 1024 / 1024:.2f} MB (float32)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. –ì—Ä–∞—Ñ–∏–∫ —É–ª—É—á—à–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if val_losses:\n",
    "    # –í—ã—á–∏—Å–ª—è–µ–º —É–ª—É—á—à–µ–Ω–∏–µ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–≤–æ–π —ç–ø–æ—Ö–∏\n",
    "    initial_val_loss = val_losses[0]\n",
    "    improvement = [(initial_val_loss - loss) / initial_val_loss * 100 for loss in val_losses]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(epochs, improvement, 'g-', linewidth=2, marker='o')\n",
    "    plt.axhline(y=0, color='r', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    plt.xlabel('–≠–ø–æ—Ö–∞', fontsize=14)\n",
    "    plt.ylabel('–£–ª—É—á—à–µ–Ω–∏–µ Val Loss (%)', fontsize=14)\n",
    "    plt.title('–ü—Ä–æ–≥—Ä–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–≤–æ–π —ç–ø–æ—Ö–∏', fontsize=16, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_improvement.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"üíæ –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω: training_improvement.png\")\n",
    "    print(f\"\\nüìà –ò—Ç–æ–≥–æ–≤–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ: {improvement[-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. –≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ CSV\n",
    "df.to_csv('training_results.csv', index=False, encoding='utf-8')\n",
    "print(\"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: training_results.csv\")\n",
    "\n",
    "# –°–æ–∑–¥–∞—ë–º –æ—Ç—á—ë—Ç\n",
    "report = f\"\"\"\n",
    "–û–¢–ß–Å–¢ –û–ë –û–ë–£–ß–ï–ù–ò–ò –ú–û–î–ï–õ–ò –ß–ê–¢-–ë–û–¢–ê\n",
    "{'=' * 50}\n",
    "\n",
    "–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:\n",
    "  - –ú–æ–¥–µ–ª—å: Seq2Seq —Å Attention\n",
    "  - Encoder: {checkpoint['num_layers']}-—Å–ª–æ–π–Ω—ã–π LSTM\n",
    "  - Decoder: {checkpoint['num_layers']}-—Å–ª–æ–π–Ω—ã–π LSTM + Bahdanau Attention\n",
    "  - –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {checkpoint['vocab_size']:,}\n",
    "  - –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {total_params:,}\n",
    "\n",
    "–û–±—É—á–µ–Ω–∏–µ:\n",
    "  - –≠–ø–æ—Ö: {len(train_losses)}\n",
    "  - –§–∏–Ω–∞–ª—å–Ω—ã–π Train Loss: {train_losses[-1]:.4f}\n",
    "\"\"\" + (f\"  - –§–∏–Ω–∞–ª—å–Ω—ã–π Val Loss: {val_losses[-1]:.4f}\\n\" if val_losses else \"\") + (\n",
    "f\"  - –õ—É—á—à–∏–π Val Loss: {min(val_losses):.4f} (—ç–ø–æ—Ö–∞ {np.argmin(val_losses) + 1})\\n\" if val_losses else \"\"\n",
    ") + f\"\"\"\n",
    "–†–µ–∑—É–ª—å—Ç–∞—Ç:\n",
    "  ‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ\n",
    "  ‚úÖ –ì–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –≤ –±–æ—Ç–µ\n",
    "\"\"\"\n",
    "\n",
    "with open('training_report.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(\"üíæ –û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: training_report.txt\")\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
